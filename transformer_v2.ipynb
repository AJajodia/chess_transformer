{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "from collections.abc import Callable\n",
    "import tqdm\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.nn import CrossEntropyLoss\n",
    "\n",
    "from utilities import (\n",
    "    Embedding,\n",
    "    LayerNorm,\n",
    "    Dropout,\n",
    "    Linear,\n",
    "    DictReLU,\n",
    "    pbt_init,\n",
    "    pbt_update,\n",
    "    Optimizer,\n",
    "    get_dataloader_random_reshuffle,\n",
    "    evaluate_model,\n",
    "    update_model,\n",
    "    get_array_minibatch,\n",
    "    get_accuracy,\n",
    "    AdamW\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\n",
    "    \"device\": \"cpu\",\n",
    "    \"ensemble_shape\": (1,),\n",
    "    \"float_dtype\": torch.float32,\n",
    "    \"hyperparameter_raw_init_distributions\": {\n",
    "        \"dropout_p\": torch.distributions.Uniform(\n",
    "            torch.tensor(0, device=\"cpu\", dtype=torch.float32),\n",
    "            torch.tensor(.01, device=\"cpu\", dtype=torch.float32)\n",
    "        ),\n",
    "        \"epsilon\": torch.distributions.Uniform(\n",
    "            torch.tensor(-10, device=\"cpu\", dtype=torch.float32),\n",
    "            torch.tensor(-5, device=\"cpu\", dtype=torch.float32)\n",
    "        ),\n",
    "        \"first_moment_decay\": torch.distributions.Uniform(\n",
    "            torch.tensor(-3, device=\"cpu\", dtype=torch.float32),\n",
    "            torch.tensor(0, device=\"cpu\", dtype=torch.float32)\n",
    "        ),\n",
    "        \"learning_rate\": torch.distributions.Uniform(\n",
    "            torch.tensor(-5, device=\"cpu\", dtype=torch.float32),\n",
    "            torch.tensor(-1, device=\"cpu\", dtype=torch.float32)\n",
    "        ),\n",
    "        \"second_moment_decay\": torch.distributions.Uniform(\n",
    "            torch.tensor(-5, device=\"cpu\", dtype=torch.float32),\n",
    "            torch.tensor(-1, device=\"cpu\", dtype=torch.float32)\n",
    "        ),\n",
    "        \"weight_decay\": torch.distributions.Uniform(\n",
    "            torch.tensor(-5, device=\"cpu\", dtype=torch.float32),\n",
    "            torch.tensor(-1, device=\"cpu\", dtype=torch.float32)\n",
    "        )\n",
    "    },\n",
    "    \"hyperparameter_raw_perturb\": {\n",
    "        \"dropout_p\": torch.distributions.Normal(\n",
    "            torch.tensor(0, device=\"cpu\", dtype=torch.float32),\n",
    "            torch.tensor(.01, device=\"cpu\", dtype=torch.float32)\n",
    "        ),\n",
    "        \"epsilon\": torch.distributions.Normal(\n",
    "            torch.tensor(0, device=\"cpu\", dtype=torch.float32),\n",
    "            torch.tensor(1, device=\"cpu\", dtype=torch.float32)\n",
    "        ),\n",
    "        \"first_moment_decay\": torch.distributions.Normal(\n",
    "            torch.tensor(0, device=\"cpu\", dtype=torch.float32),\n",
    "            torch.tensor(1, device=\"cpu\", dtype=torch.float32)\n",
    "        ),\n",
    "        \"learning_rate\": torch.distributions.Normal(\n",
    "            torch.tensor(0, device=\"cpu\", dtype=torch.float32),\n",
    "            torch.tensor(1, device=\"cpu\", dtype=torch.float32)\n",
    "        ),\n",
    "        \"second_moment_decay\": torch.distributions.Normal(\n",
    "            torch.tensor(0, device=\"cpu\", dtype=torch.float32),\n",
    "            torch.tensor(1, device=\"cpu\", dtype=torch.float32)\n",
    "        ),\n",
    "        \"weight_decay\": torch.distributions.Normal(\n",
    "            torch.tensor(0, device=\"cpu\", dtype=torch.float32),\n",
    "            torch.tensor(1, device=\"cpu\", dtype=torch.float32)\n",
    "        ),\n",
    "    },\n",
    "    \"hyperparameter_transforms\": {\n",
    "        \"dropout_p\": lambda p: p.clip(0,1),\n",
    "        \"epsilon\": lambda log10: 10 ** log10,\n",
    "        \"first_moment_decay\": lambda x: (1 - 10 ** x).clamp(0, 1),\n",
    "        \"learning_rate\": lambda log10: 10 ** log10,\n",
    "        \"second_moment_decay\": lambda x: (1 - 10 ** x).clamp(0, 1),\n",
    "        \"weight_decay\": lambda log10: 10 ** log10,\n",
    "    },\n",
    "    \"improvement_threshold\": 1e-4,\n",
    "    \"minibatch_size\": 100,\n",
    "    \"pbt\": True,\n",
    "    \"seed\": 1,\n",
    "    \"sequence_size\": 64,\n",
    "    \"steps_num\": 10000,\n",
    "    \"steps_without_improvement\": 1_000,\n",
    "    \"valid_interval\": 100,\n",
    "    \"welch_confidence_level\": .95,\n",
    "    \"welch_sample_size\": 1024,\n",
    "    \"embedding_dim\": 36,\n",
    "    \"dropout_p\": torch.tensor([1], device='mps'),\n",
    "    \"n_heads\": 3,\n",
    "    \"block_num\": 2,\n",
    "    \"minibatch_size_eval\": 100\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x130352ad0>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.manual_seed(config[\"seed\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_positional_embeddings(sequence_length, d):\n",
    "    result = torch.ones(sequence_length, d, device=config[\"device\"])\n",
    "    for i in range(sequence_length):\n",
    "        for j in range(d):\n",
    "            result[i][j] = np.sin(i / (10000 ** (j / d))) if j % 2 == 0 else np.cos(i / (10000 ** ((j - 1) / d)))\n",
    "    return result\n",
    "\n",
    "pos_embed = get_positional_embeddings(64, config[\"embedding_dim\"])[None, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EmbeddingBlock(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        config,\n",
    "        embedding_dim\n",
    "    ):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.piece_embed = Embedding(config, embedding_dim, vocabulary_size=7)\n",
    "        self.player_embed = Embedding(config, embedding_dim, vocabulary_size=2)\n",
    "        self.dropout = Dropout(config)\n",
    "        self.pos_embed = pos_embed\n",
    "        \n",
    "    def forward(self, input):\n",
    "        input = input[0]\n",
    "        embedded_pieces = self.dropout({'features': self.piece_embed(input[...,0])})\n",
    "        embedded_players = self.dropout({'features': self.player_embed(input[...,1])})\n",
    "        embedded_pos = self.pos_embed\n",
    "        \n",
    "        output = embedded_pieces['features'] + embedded_players['features'] + embedded_pos\n",
    "        \n",
    "        return {'features': output.to(config[\"device\"]), 'mask': torch.ones((config['minibatch_size'], config['sequence_size']), device=config['device']).to(torch.bool)}\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadSelfAttentionBlock(nn.Module):\n",
    "    \"\"\"\n",
    "    Pre-LN multi-head self-attention block.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    attention_head_num : `int`\n",
    "        The number of attention heads.\n",
    "    config : `int`\n",
    "        Configuration dictionary. Required key-value pairs:\n",
    "        `\"device\"` : `str`\n",
    "            The device to store parameters on.\n",
    "        `\"dropout_p\"` : `torch.Tensor`\n",
    "            Dropout probability tensor, of shape `ensemble_shape`.\n",
    "        `\"ensemble_shape\"` : `tuple[int]`\n",
    "            Ensemble shape.      \n",
    "        `\"float_dtype\"` : `torch.dtype`\n",
    "            The floating point datatype to use for the parameters.\n",
    "    embedding_dim : `int`\n",
    "        The feature dimension of internal representations.\n",
    "\n",
    "    Calling\n",
    "    -------\n",
    "    Instance calls require one positional argument:\n",
    "    batch : `dict`\n",
    "        The input data dictionary. Required keys:\n",
    "        `\"features\"` : `torch.Tensor`\n",
    "            Tensor of element-level features, of shape\n",
    "            `batch_shape + (sequence_dim, embedding_dim)` or\n",
    "            `ensemble_shape + batch_shape + (sequence_dim, embedding_dim)`\n",
    "        `\"mask\"` : `torch.Tensor`\n",
    "            Mask showing which entries are not padding, of shape\n",
    "            `batch_shape + (sequence_dim,)` or\n",
    "            `ensemble_shape + batch_shape + (sequence_dim,)`\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        attention_head_num: int,\n",
    "        config: dict,\n",
    "        embedding_dim: int\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        self.attention_head_num = attention_head_num\n",
    "        self.config = config\n",
    "        self.dropout = Dropout(config)\n",
    "        self.layer_norm = LayerNorm(\n",
    "            config,\n",
    "            embedding_dim\n",
    "        )\n",
    "\n",
    "        (\n",
    "            self.key_weights,\n",
    "            self.output_weights,\n",
    "            self.query_weights,\n",
    "            self.value_weights\n",
    "        ) = (\n",
    "            Linear(\n",
    "                config,\n",
    "                embedding_dim,\n",
    "                embedding_dim,\n",
    "                bias=False\n",
    "            )\n",
    "            for _ in range(4)\n",
    "        )\n",
    "\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        batch: dict\n",
    "    ) -> dict:\n",
    "        skip = batch[\"features\"]\n",
    "        batch = self.layer_norm(batch)\n",
    "        residual, mask = (batch[key] for key in (\"features\", \"mask\"))\n",
    "\n",
    "        sequence_dim, embedding_dim = residual.shape[-2:]\n",
    "        key_dim = embedding_dim // self.attention_head_num\n",
    "\n",
    "        key, query, value = (\n",
    "            (\n",
    "                linear(residual)\n",
    "            ).reshape(\n",
    "                residual.shape[:-1] + (self.attention_head_num, key_dim)\n",
    "            ).transpose(-3, -2)\n",
    "            for linear in (\n",
    "                self.key_weights,\n",
    "                self.query_weights,\n",
    "                self.value_weights\n",
    "            )\n",
    "        )\n",
    "        \n",
    "        \n",
    "        arange = torch.arange(sequence_dim, device=mask.device)\n",
    "        attention_mask = mask[..., None, :] & mask[..., None]\n",
    "        attention_mask |= (arange == arange[:, None])\n",
    "\n",
    "        pooled_values = F.scaled_dot_product_attention(\n",
    "            query,\n",
    "            key,\n",
    "            value,\n",
    "            attention_mask[..., None, :, :]\n",
    "        )\n",
    "\n",
    "        residual = pooled_values.transpose(-3, -2).reshape(residual.shape)\n",
    "        residual = self.output_weights(residual)\n",
    "        residual = self.dropout({\"features\": residual})[\"features\"]\n",
    "\n",
    "        features = skip + residual\n",
    "\n",
    "        return batch | {\"features\": features}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedForwardBlock(torch.nn.Module):\n",
    "    \"\"\"\n",
    "    Pre-LN feedforward block.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    config : `int`\n",
    "        Configuration dictionary. Required key-value pairs:\n",
    "        `\"device\"` : `str`\n",
    "            The device to store parameters on.\n",
    "        `\"dropout_p\"` : `torch.Tensor`\n",
    "            Dropout probability tensor, of shape `ensemble_shape`.\n",
    "        `\"ensemble_shape\"` : `tuple[int]`\n",
    "            Ensemble shape.      \n",
    "        `\"float_dtype\"` : `torch.dtype`\n",
    "            The floating point datatype to use for the parameters.\n",
    "    embedding_dim : `int`\n",
    "        The feature dimension of internal representations.\n",
    "\n",
    "    Calling\n",
    "    -------\n",
    "    Instance calls require one positional argument:\n",
    "    batch : `dict`\n",
    "        The input data dictionary. Required key:\n",
    "        `\"features\"` : `torch.Tensor`\n",
    "            Tensor of element-level features, of shape\n",
    "            `batch_shape + (sequence_dim, embedding_dim)` or\n",
    "            `ensemble_shape + batch_shape + (sequence_dim, embedding_dim)`\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        config: dict,\n",
    "        embedding_dim: int\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        self.residual_f = torch.nn.Sequential(\n",
    "            LayerNorm(\n",
    "                config,\n",
    "                embedding_dim\n",
    "            ),\n",
    "            Linear(\n",
    "                config,\n",
    "                embedding_dim,\n",
    "                embedding_dim,\n",
    "                init_multiplier=2 ** .5\n",
    "            ),\n",
    "            DictReLU(),\n",
    "            Linear(\n",
    "                config,\n",
    "                embedding_dim,\n",
    "                embedding_dim\n",
    "            ),\n",
    "            Dropout(config)\n",
    "        )\n",
    "\n",
    "\n",
    "    def forward(self, batch: dict) -> dict:\n",
    "        skip = batch\n",
    "\n",
    "        residual = self.residual_f(batch)\n",
    "\n",
    "        features = skip[\"features\"] + residual[\"features\"]\n",
    "\n",
    "        return batch | {\"features\": features}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_supervised(\n",
    "    config: dict,\n",
    "    dataset_train: dict,\n",
    "    dataset_valid: dict,\n",
    "    get_loss: Callable[[dict, torch.Tensor], torch.Tensor],\n",
    "    get_metric: Callable[[dict, torch.Tensor], torch.Tensor],\n",
    "    model: torch.nn.Module,\n",
    "    optimizer: Optimizer,\n",
    "    target_key=\"target\"\n",
    ") -> dict:\n",
    "    \"\"\"\n",
    "    Population-based training on a supervised learning task.\n",
    "    Tuned hyperparameters are given by raw values and transformations.\n",
    "    This way, the hyperparameters are perturbed by\n",
    "    additive noise on raw values.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    config : `dict`\n",
    "        Configuration dictionary. Required key-value pairs:\n",
    "        `\"ensemble_shape\"` : tuple[int]\n",
    "            Ensemble shape. We assume this is a 1-dimensional tuple\n",
    "            with dimensions the population size.\n",
    "        `\"hyperparameter_raw_init_distributions\"` : `dict`\n",
    "            Dictionary that maps tuned hyperparameter names\n",
    "            to `torch.distributions.Distribution` of raw hyperparameter values.\n",
    "            Required keys:\n",
    "            `\"learning_rate\"`:\n",
    "                The learning rate of stochastic gradient descent.\n",
    "        `\"hyperparameter_raw_perturbs\"` : `dict`\n",
    "            Dictionary that maps tuned hyperparameter names\n",
    "            to `torch.distributions.Distribution` of additive noise.\n",
    "        `\"hyperparameter_transforms\"` : `dict`\n",
    "            Dictionary that maps tuned hyperparameter names\n",
    "            to transformations of raw hyperparameter values.\n",
    "        `\"improvement_threshold\"` : `float`\n",
    "            A new metric score has to be this much better\n",
    "            than the previous best to count as an improvement.\n",
    "        `\"minibatch_size\"` : `int`\n",
    "            Minibatch size to use in a training step.\n",
    "        `\"minibatch_size_eval\"` : `int`\n",
    "            Minibatch size to use in evaluation.\n",
    "            On CPU, should be about the same as `minibatch_size`.\n",
    "            On GPU, should be as big as possible without\n",
    "            incurring an Out of Memory error.\n",
    "        `\"pbt\"` : `bool`\n",
    "            Whether to use PBT updates in validations.\n",
    "            If `False`, the algorithm just samples hyperparameters at start,\n",
    "            then keeps them constant.\n",
    "        `\"steps_num\"` : `int`\n",
    "            Maximum number of training steps.\n",
    "        `\"steps_without_improvement`\" : `int`\n",
    "            If the number of training steps without improvement\n",
    "            exceeds this value, then training is stopped.\n",
    "        `\"valid_interval\"` : `int`\n",
    "            Frequency of evaluations, measured in number of training steps.\n",
    "        `\"welch_confidence_level\"` : `float`\n",
    "            The confidence level in Welch's t-test\n",
    "            that is used in determining if a population member\n",
    "            is to be replaced by another member with perturbed hyperparameters.\n",
    "        `\"welch_sample_size\"` : `int`\n",
    "            The last this many validation metrics are used\n",
    "            in Welch's t-test.\n",
    "    dataset_train : `dict`\n",
    "        The dataset to train the model on.\n",
    "    dataset_valid : `dict`\n",
    "        The dataset to evaluate the model on.\n",
    "    `get_loss` : `Callable[[dict, torch.Tensor], torch.Tensor]`\n",
    "        A function that maps a pair of model output and target value tensor\n",
    "        to a tensor of losses per ensemble member.\n",
    "    `get_metric` : `Callable[[dict, torch.Tensor], torch.Tensor]`\n",
    "        A function that maps a pair of model output and target value tensor\n",
    "        to a tensor of metrics per ensemble member.\n",
    "        We assume a greater metric is better.\n",
    "    `model` : `torch.nn.Module`\n",
    "        The model ensemble to tune.\n",
    "    `optimizer` : `Optimizer`\n",
    "        An optimizer that tracks the parameters of `model`.\n",
    "    indptr_key : `str`, optional\n",
    "        If the dataset has sequential entries,\n",
    "        then this is the key of the index pointer tensor.\n",
    "        Default: `\"indptr\"`.\n",
    "    target_key : `str`, optional\n",
    "        The key mapped to the target value tensor in the dataset.\n",
    "        Default: `\"target\"`\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    An output dictionary with the following key-value pairs:\n",
    "        `\"source mask\"` : `torch.Tensor`\n",
    "            The source masks of population members\n",
    "            that were replace by other members in a PBT update\n",
    "        `\"target indices\"` : `torch.Tensor`\n",
    "            The indices of population members\n",
    "            that the member where the source mask is to were replaced with.\n",
    "        `\"validation metric\"` : `torch.Tensor`\n",
    "            The validation metrics at evaluation steps.\n",
    "\n",
    "        In addition, for each tuned hyperparameter name,\n",
    "        we include a `torch.Tensor` of values per update.\n",
    "    \"\"\"\n",
    "    ensemble_shape = config[\"ensemble_shape\"]\n",
    "    if len(ensemble_shape) != 1:\n",
    "        raise ValueError(f\"The number of dimensions in the ensemble shape should be 1 for the  population size, but it is {len(ensemble_shape)}\")\n",
    "\n",
    "    config_local = dict(config)\n",
    "    log = defaultdict(list)\n",
    "\n",
    "    pbt_init(config_local, log)\n",
    "\n",
    "    update_model(config_local, model)\n",
    "    optimizer.update_config(config_local)\n",
    "\n",
    "    best_valid_metric = -torch.inf\n",
    "    progress_bar = tqdm.trange(config[\"steps_num\"])\n",
    "    steps_without_improvement = 0\n",
    "    train_dataloader = get_dataloader_random_reshuffle(\n",
    "        config,\n",
    "        dataset_train[\"features\"],\n",
    "        dataset_train[\"labels\"]\n",
    "    )\n",
    "\n",
    "    for step_id in progress_bar:\n",
    "        model.train()\n",
    "        minibatch = next(train_dataloader)\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        predict = model(minibatch[0])[0][...,0]\n",
    "        target = minibatch[1][0]\n",
    "\n",
    "        loss = get_loss(predict, target).sum()\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        if step_id % config[\"valid_interval\"] == 0:\n",
    "            model.eval()\n",
    "            with torch.no_grad():\n",
    "                split_name = \"validation\"\n",
    "                minibatch = next(train_dataloader)\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                predict = model(minibatch[0])[0][...,0]\n",
    "                target = minibatch[1][0]\n",
    "                \n",
    "                print(predict.argmax(dim=-1))\n",
    "                print(target)\n",
    "                \n",
    "                metric = (predict.argmax(dim=-1) == target).to(config[\"float_dtype\"]).mean()\n",
    "                \n",
    "                # log[f\"{split_name} loss\"].append(loss)\n",
    "                log[f\"{split_name} metric\"].append(metric)\n",
    "                # print(\n",
    "                #     f\"{split_name} loss {loss.min().cpu().item():.4f}\"\n",
    "                # )\n",
    "                print(\n",
    "                    f\"{split_name} metric {metric.max().cpu().item():.4f}\"\n",
    "                )\n",
    "\n",
    "                best_last_metric = log[\"validation metric\"][-1].max()\n",
    "                print(\n",
    "                    f\"Best last metric {best_last_metric.cpu().item():.2f}\",\n",
    "                    flush=True\n",
    "                )\n",
    "                if (\n",
    "                    best_valid_metric + config[\"improvement_threshold\"]\n",
    "                ) < best_last_metric:\n",
    "                    print(\n",
    "                        f\"New best metric\",\n",
    "                        flush=True\n",
    "                    )\n",
    "                    best_valid_metric = best_last_metric\n",
    "                    steps_without_improvement = 0\n",
    "                else:\n",
    "                    print(\n",
    "                        f\"Best metric {best_valid_metric.cpu().item():.2f}\",\n",
    "                        flush=True\n",
    "                    )\n",
    "                    steps_without_improvement += config[\"valid_interval\"]\n",
    "                    if steps_without_improvement > config[\n",
    "                        \"steps_without_improvement\"\n",
    "                    ]:\n",
    "                        break\n",
    "\n",
    "                if config[\"pbt\"] and (len(log[\"validation metric\"]) >= config[\n",
    "                    \"welch_sample_size\"\n",
    "                ]):\n",
    "                    evaluations = torch.stack(\n",
    "                        log[\"validation metric\"][-config[\"welch_sample_size\"]:]\n",
    "                    )\n",
    "                    pbt_update(\n",
    "                        config_local, evaluations, log, optimizer.get_parameters()\n",
    "                    )\n",
    "\n",
    "                    update_model(config_local, model)\n",
    "                    optimizer.update_config(config_local)\n",
    "\n",
    "\n",
    "    progress_bar.close()\n",
    "    for key, value in log.items():\n",
    "        if isinstance(value, list):\n",
    "            log[key] = torch.stack(value)\n",
    "\n",
    "    return log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super(Transformer, self).__init__()\n",
    "        \n",
    "        embedding_dim = config[\"embedding_dim\"]\n",
    "        n_heads = config[\"n_heads\"]\n",
    "        block_num = config[\"block_num\"]\n",
    "        \n",
    "        # create embedding block\n",
    "        self.EmbeddingBlock = EmbeddingBlock(config, embedding_dim)\n",
    "        \n",
    "        # make repeated transformer block\n",
    "        blocks = []\n",
    "        for _ in range(block_num):\n",
    "            blocks.extend([\n",
    "                MultiHeadSelfAttentionBlock(n_heads, config, embedding_dim),\n",
    "                FeedForwardBlock(config, embedding_dim)\n",
    "            ])\n",
    "        self.MHA_FF_Block = nn.Sequential(*blocks)\n",
    "        \n",
    "        # use Linear to create logits\n",
    "        self.Linear = Linear(config, embedding_dim, 1)\n",
    "        \n",
    "        # use softmax\n",
    "        self.softmax = nn.Softmax()\n",
    "        \n",
    "\n",
    "    def forward(self, input):\n",
    "        embedded_inputs = self.EmbeddingBlock(input)\n",
    "        \n",
    "        out = self.MHA_FF_Block(embedded_inputs)\n",
    "        \n",
    "        logits = self.Linear(out)[\"features\"]\n",
    "        \n",
    "        logits = self.softmax(logits)\n",
    "        \n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = torch.load('data/dataset_v2.pt', map_location=config[\"device\"])\n",
    "dataset_train, dataset_valid = {}, {}\n",
    "dataset_train[\"features\"], dataset_valid[\"features\"], dataset_train[\"labels\"], dataset_valid[\"labels\"] = train_test_split(\n",
    "    dataset[\"features\"], dataset[\"labels\"][:,0], test_size=0.2, random_state=config['seed'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[4, 1],\n",
       "        [0, 0],\n",
       "        [0, 0],\n",
       "        [0, 0],\n",
       "        [0, 0],\n",
       "        [4, 1],\n",
       "        [6, 1],\n",
       "        [0, 0],\n",
       "        [1, 1],\n",
       "        [1, 1],\n",
       "        [1, 1],\n",
       "        [5, 1],\n",
       "        [0, 0],\n",
       "        [1, 1],\n",
       "        [1, 1],\n",
       "        [1, 1],\n",
       "        [0, 0],\n",
       "        [0, 0],\n",
       "        [2, 1],\n",
       "        [0, 0],\n",
       "        [0, 0],\n",
       "        [2, 1],\n",
       "        [0, 0],\n",
       "        [0, 0],\n",
       "        [0, 0],\n",
       "        [0, 0],\n",
       "        [3, 1],\n",
       "        [1, 1],\n",
       "        [1, 1],\n",
       "        [0, 0],\n",
       "        [0, 0],\n",
       "        [0, 0],\n",
       "        [0, 0],\n",
       "        [0, 0],\n",
       "        [0, 0],\n",
       "        [0, 0],\n",
       "        [0, 0],\n",
       "        [0, 0],\n",
       "        [3, 1],\n",
       "        [0, 0],\n",
       "        [0, 0],\n",
       "        [1, 0],\n",
       "        [0, 0],\n",
       "        [1, 0],\n",
       "        [1, 0],\n",
       "        [0, 0],\n",
       "        [1, 0],\n",
       "        [0, 0],\n",
       "        [1, 0],\n",
       "        [3, 0],\n",
       "        [1, 0],\n",
       "        [5, 0],\n",
       "        [2, 0],\n",
       "        [1, 0],\n",
       "        [3, 0],\n",
       "        [1, 0],\n",
       "        [4, 0],\n",
       "        [2, 0],\n",
       "        [0, 0],\n",
       "        [0, 0],\n",
       "        [6, 0],\n",
       "        [0, 0],\n",
       "        [0, 0],\n",
       "        [4, 0]])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_train[\"features\"][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/10000 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([59, 38, 59, 13,  4, 11, 59, 44, 16, 18, 36,  1, 25, 52, 52, 59, 16, 52,\n",
      "        55,  6, 28, 15, 36, 16, 36, 56, 30, 60, 19, 45, 11, 49, 59, 43, 36, 52,\n",
      "        26, 46, 58, 51, 13, 47, 11,  0, 36, 13, 51, 50, 49, 59, 47,  5, 33, 12,\n",
      "        28, 59, 20, 62, 61, 59, 31, 61, 61, 46, 41,  5,  6, 34, 27,  2, 59, 55,\n",
      "        51, 54, 39, 51,  7, 51, 46, 55, 58, 59,  6, 18, 59,  4, 57, 49, 11, 18,\n",
      "        24, 36,  4,  6,  6, 36, 31, 12, 29, 63])\n",
      "tensor([57, 62, 54, 10,  4, 50, 59, 38, 34, 51, 13, 18, 26,  5, 11, 52, 52, 27,\n",
      "        21, 51, 30, 38, 46, 50,  3, 12, 31, 29, 52, 26, 60, 14, 61, 55,  3,  2,\n",
      "        32, 14, 26,  3, 60, 21, 54, 36,  8, 26,  3, 11,  3, 30, 52, 38, 27, 49,\n",
      "         3, 59, 16, 19, 17, 62, 17, 40, 24, 26, 35, 38,  2, 36, 45, 42, 34,  2,\n",
      "         5, 23, 60, 29, 38,  3, 58, 27, 51, 50, 14, 36, 52, 55, 38, 12, 52, 41,\n",
      "        32, 28, 52, 10, 28,  5, 21, 47, 43, 26])\n",
      "validation metric 0.0300\n",
      "Best last metric 0.03\n",
      "New best metric\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 99/10000 [00:05<08:31, 19.34it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([59, 26, 41, 19, 12, 12, 11, 51, 56, 59, 59,  3,  0, 51,  7,  3, 59, 27,\n",
      "        51, 52, 18, 57,  6, 44,  1,  2,  8, 45, 25, 15, 48,  3, 47, 28, 28, 56,\n",
      "        59, 59, 28, 22, 10, 42,  3, 28, 59, 33, 23, 52, 33,  0,  3,  3,  3, 30,\n",
      "         0, 46, 59,  3, 53, 60, 61, 18, 59,  4, 59, 11, 43, 39,  3, 13,  6, 45,\n",
      "        52, 21, 51, 32,  2,  3, 16,  3, 36, 49,  0, 38,  3, 59, 51,  3,  3, 14,\n",
      "        28, 12, 29,  3, 34,  3, 59, 54, 59,  2])\n",
      "tensor([30, 26, 29, 12,  5, 12,  8, 56, 56, 51, 46, 36,  0, 50,  9,  1, 59, 29,\n",
      "        61, 58, 18, 41,  4, 44,  1, 21, 44, 41, 48,  0, 44,  9,  5, 56, 28, 56,\n",
      "        25, 52, 56, 22, 58, 26, 21, 14, 58,  9, 23, 60, 18, 10, 11, 27,  6, 23,\n",
      "        15, 55, 52, 38,  5, 51, 29, 14, 55,  4, 39, 11, 43, 35, 26, 38,  4, 56,\n",
      "        29, 36, 53, 49, 38, 11, 23,  3, 52, 63, 27, 56, 13, 55, 42,  2,  3, 20,\n",
      "        56, 19, 29,  1, 55, 12, 57, 63, 35,  4])\n",
      "validation metric 0.1800\n",
      "Best last metric 0.18\n",
      "New best metric\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 198/10000 [00:09<07:51, 20.81it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 0, 56, 59, 32, 56, 24, 59,  0,  0,  0, 63, 23, 60, 14,  0,  0,  8, 18,\n",
      "        33, 62, 40, 22, 11, 11, 56, 61, 59, 59, 62,  0,  0, 57, 26, 54, 59,  0,\n",
      "         0, 53,  8, 59, 10, 60, 38, 59,  0, 62, 59, 62,  4,  0, 49, 62,  3,  0,\n",
      "        12, 31,  0, 59, 59,  1, 21, 43, 15, 56, 59, 59, 44,  7, 61, 22,  0, 42,\n",
      "        59, 59,  0,  0,  6, 59,  0,  0, 43, 32, 59, 19, 59,  0, 59, 58, 60, 43,\n",
      "         9, 33, 60, 59, 29,  0, 59,  0,  0,  5])\n",
      "tensor([26, 28, 62, 32, 41,  6, 56,  3,  1, 21, 47, 27, 57, 14, 21, 11,  8, 18,\n",
      "        11, 57, 54,  4, 11,  2, 34, 52, 49, 41, 48, 11,  2, 20, 26, 39, 46,  5,\n",
      "         5, 53, 28, 61, 10, 51, 16, 61,  4, 41, 52, 34,  4,  4, 39, 39, 13, 15,\n",
      "        12, 27,  0, 44,  9,  9, 21, 43, 18, 45, 58, 62,  5, 19, 47, 27, 20, 42,\n",
      "        58, 45, 29,  2,  4, 59,  1, 22, 41, 26, 60, 31, 60,  8, 62,  6, 58, 55,\n",
      "        25, 19, 45, 51, 48,  6, 48, 14,  1,  2])\n",
      "validation metric 0.1500\n",
      "Best last metric 0.15\n",
      "Best metric 0.18\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  3%|▎         | 300/10000 [00:14<07:46, 20.79it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([54, 56, 28,  7, 18, 16, 25,  7, 44, 56, 50,  3, 56, 56, 19,  1, 62, 12,\n",
      "        29, 11,  7, 17, 62, 26, 56,  7, 63, 14, 13, 58, 29, 54, 22, 55,  7,  8,\n",
      "        56, 25, 20, 56, 22, 41, 22, 29,  7, 25, 27, 49, 51, 58, 59, 14, 38, 56,\n",
      "         5, 14, 56,  9, 10, 53, 39, 63,  5, 24, 51, 15, 47, 62,  5, 52, 16, 32,\n",
      "         0, 42, 59, 58, 40, 16,  7, 33, 56,  6,  7, 40,  5, 14, 52, 56, 14, 56,\n",
      "         7, 28, 56, 30,  7, 56, 23, 52, 29, 16])\n",
      "tensor([54, 61, 61, 13, 26,  4, 25, 14, 27, 45, 46, 34, 58, 60, 55,  2, 43, 36,\n",
      "        15, 27, 12,  4, 62, 23, 41, 10, 43,  6,  4, 45,  3,  6, 27, 37, 36,  8,\n",
      "        57, 18, 57, 60,  5, 13, 20, 16, 10, 25, 37,  8, 54, 61, 52, 26,  1, 52,\n",
      "        26,  3, 61,  0, 31, 48, 43, 62,  9, 24, 40, 35, 37, 60, 14, 46, 43, 32,\n",
      "         2, 44, 60, 42, 40, 11, 12, 59, 51, 18, 15, 61,  5,  4, 48, 61,  2, 51,\n",
      "        22, 11, 44, 30, 12, 60, 31, 39, 12, 18])\n",
      "validation metric 0.1000\n",
      "Best last metric 0.10\n",
      "Best metric 0.18\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|▍         | 399/10000 [00:19<07:41, 20.82it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 0, 55, 48, 25, 39, 57,  3,  0, 43, 23, 57, 20, 58,  5,  0,  6, 21, 63,\n",
      "        17, 57, 35, 10, 56,  9, 32, 18,  6,  1,  7, 62, 58,  5,  5, 55, 60, 57,\n",
      "        63,  0, 35, 28, 46, 55, 16, 26,  0, 41, 13,  6, 11, 40,  2, 57, 49, 24,\n",
      "        55, 57, 18, 55, 57, 55, 20, 50,  6, 63, 43, 63,  0,  0,  0,  0,  8, 63,\n",
      "         4,  5,  0, 56, 63,  0,  0, 27,  0,  0, 38, 55, 57, 55, 57, 18, 57,  6,\n",
      "        31,  0, 61, 57,  5,  0, 52, 55, 42, 28])\n",
      "tensor([ 1, 44, 51, 25, 52, 62,  3,  6, 54, 29, 60, 20, 37, 24,  5,  5, 21, 53,\n",
      "        25, 50, 19, 44, 34, 62, 32, 18, 50, 19,  4, 45, 59, 19, 26, 38, 43, 62,\n",
      "        51, 29, 53, 49, 58, 52, 11, 35,  1, 46, 33, 21, 22, 10, 37, 55, 49, 25,\n",
      "        51, 55,  0, 62, 49, 51, 36, 49, 10, 62, 27, 48,  8,  6, 12, 11, 12, 52,\n",
      "        19, 22, 14, 56, 46, 18, 21, 27,  9, 52, 47, 51, 61, 42, 49,  4, 25, 35,\n",
      "        31,  5, 62, 50, 30,  6, 14, 53, 17, 55])\n",
      "validation metric 0.1000\n",
      "Best last metric 0.10\n",
      "Best metric 0.18\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  5%|▍         | 499/10000 [00:24<08:06, 19.53it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([48, 23,  3, 44, 47, 49,  3,  1, 44, 62, 56, 13, 11,  3, 19,  3, 33,  3,\n",
      "         1,  3,  1,  2,  1,  1, 63, 18, 63,  4, 56, 17, 63, 57, 24, 38, 16, 63,\n",
      "        36, 20,  0,  1, 62, 40, 60, 63, 58, 16, 16, 15,  3, 63, 16,  8, 32, 42,\n",
      "        44, 18,  7, 30,  3, 63, 40,  3, 63, 59, 28,  3, 37, 16, 63, 63, 31, 41,\n",
      "        30, 52,  1,  3, 53,  3, 46,  3, 36, 35, 63, 63,  3, 42, 63, 41, 62, 57,\n",
      "         3,  6, 16, 44, 52, 39, 34, 14,  5, 41])\n",
      "tensor([12, 19,  5, 48, 47, 39,  2,  2, 34, 51, 17, 27, 11, 26, 53,  3, 33,  6,\n",
      "        10,  2, 10, 22, 12, 52, 52, 27, 55, 24, 48, 17, 59, 60, 47, 21, 17, 60,\n",
      "        56, 37,  9,  1, 62, 37, 51, 52, 60, 13, 33, 36,  9, 28,  1,  8, 52, 15,\n",
      "         2, 21, 28, 59,  5, 50, 51, 14, 58, 61, 19,  8, 49, 11, 44, 30,  0, 41,\n",
      "        18, 13, 26,  6, 39, 29, 58, 26, 50, 45, 62, 51, 11, 43, 28, 52, 59, 50,\n",
      "        12, 21, 61, 41, 45,  0, 39, 14, 13, 50])\n",
      "validation metric 0.1000\n",
      "Best last metric 0.10\n",
      "Best metric 0.18\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  6%|▌         | 599/10000 [00:29<07:30, 20.88it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 4, 50, 23, 59, 41, 21,  0, 28, 57, 17,  0, 58, 10, 59, 51, 36,  3,  0,\n",
      "        63,  8, 45, 38, 39, 51, 57, 18,  0, 11,  4, 34, 29, 60, 44, 49,  0, 18,\n",
      "        26, 60, 53, 59, 40, 48, 57, 11,  7, 25,  3, 31, 12, 59, 57,  0,  6, 11,\n",
      "        46, 43, 52, 41, 59, 57, 42,  0,  9, 57, 47, 23, 56, 54, 59, 32, 14, 52,\n",
      "        31,  0,  0, 52, 19,  0, 10,  0,  2, 24, 57, 22, 57, 27, 57,  0, 17, 30,\n",
      "        59, 29, 42,  1, 24, 62, 44, 52, 35, 52])\n",
      "tensor([ 4, 60, 49,  5, 45, 21, 10, 28, 53,  6,  0, 26,  5, 58, 45,  5, 16,  5,\n",
      "         3, 45, 45, 13, 54, 51, 57, 41, 21, 11, 36, 40, 55, 60, 55, 49, 13,  5,\n",
      "        27, 42, 36, 60, 54, 60, 51, 11,  7, 25, 23, 13, 12, 45, 62,  5, 13, 33,\n",
      "        46, 43, 13, 10, 61, 45, 45, 20, 56, 52, 31, 54, 15, 55, 58, 32, 14, 55,\n",
      "        31,  4, 26, 44, 19, 35,  0,  9,  3, 38, 57, 22, 61,  0, 52, 11, 17, 13,\n",
      "        36,  2, 52, 55, 24, 14, 54, 45, 35, 63])\n",
      "validation metric 0.2500\n",
      "Best last metric 0.25\n",
      "New best metric\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  7%|▋         | 698/10000 [00:34<07:27, 20.76it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([38, 34, 58, 35, 56, 59, 56, 22, 21, 59,  0, 56, 56, 46, 16, 45,  4, 56,\n",
      "        21, 56, 45,  2, 31, 18, 52, 56, 21, 52,  6, 35, 21, 21, 54, 11, 56, 49,\n",
      "        61,  5, 20, 29,  6,  6, 61,  6, 46,  1, 17,  0, 21, 33, 41, 54, 35,  9,\n",
      "        23, 50,  6, 52, 30, 21, 35, 56, 41, 56, 24, 43, 25, 56, 56, 21, 19, 56,\n",
      "        10, 21,  7, 19, 60,  6, 35,  3,  0, 12, 30, 57,  4, 56, 21, 41, 58, 44,\n",
      "        11, 27, 41,  8, 21, 21, 40, 21,  6,  0])\n",
      "tensor([59, 34, 51, 35, 61, 45, 44,  9, 21, 50, 20, 35, 58, 51, 31, 44, 24, 35,\n",
      "         4, 62, 56, 24, 15, 24, 61, 63, 12, 38, 10, 35, 38,  1, 54, 35, 36, 56,\n",
      "        37,  5, 20, 10,  3, 10, 29, 10, 55, 26, 42,  5,  5, 62, 61, 20, 30, 44,\n",
      "        33, 57,  6, 37, 30,  5, 35, 44, 40, 51, 24, 42, 25, 58, 51, 14, 62, 60,\n",
      "        10, 12, 37,  2,  0,  6, 60, 28,  2, 12, 37, 62, 36, 59, 18, 46, 26, 44,\n",
      "        22, 13, 52, 43,  4, 35, 40,  2, 25,  9])\n",
      "validation metric 0.1700\n",
      "Best last metric 0.17\n",
      "Best metric 0.25\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  7%|▋         | 736/10000 [00:36<07:38, 20.21it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m model \u001b[38;5;241m=\u001b[39m Transformer(config)\u001b[38;5;241m.\u001b[39mto(config[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdevice\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[1;32m      2\u001b[0m optimizer \u001b[38;5;241m=\u001b[39m AdamW(model\u001b[38;5;241m.\u001b[39mparameters())\n\u001b[0;32m----> 4\u001b[0m log \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_supervised\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[43m    \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdataset_train\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdataset_valid\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[43m    \u001b[49m\u001b[43mCrossEntropyLoss\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[43m    \u001b[49m\u001b[43mget_accuracy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     10\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     11\u001b[0m \u001b[43m    \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     12\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtarget_key\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mlabels\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\n\u001b[1;32m     13\u001b[0m \u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[8], line 128\u001b[0m, in \u001b[0;36mtrain_supervised\u001b[0;34m(config, dataset_train, dataset_valid, get_loss, get_metric, model, optimizer, target_key)\u001b[0m\n\u001b[1;32m    125\u001b[0m minibatch \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mnext\u001b[39m(train_dataloader)\n\u001b[1;32m    126\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m--> 128\u001b[0m predict \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mminibatch\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m,\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    129\u001b[0m target \u001b[38;5;241m=\u001b[39m minibatch[\u001b[38;5;241m1\u001b[39m][\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    131\u001b[0m loss \u001b[38;5;241m=\u001b[39m get_loss(predict, target)\u001b[38;5;241m.\u001b[39msum()\n",
      "File \u001b[0;32m~/miniforge3/envs/deep_learning/lib/python3.12/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniforge3/envs/deep_learning/lib/python3.12/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[9], line 31\u001b[0m, in \u001b[0;36mTransformer.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[1;32m     29\u001b[0m     embedded_inputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mEmbeddingBlock(\u001b[38;5;28minput\u001b[39m)\n\u001b[0;32m---> 31\u001b[0m     out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mMHA_FF_Block\u001b[49m\u001b[43m(\u001b[49m\u001b[43membedded_inputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     33\u001b[0m     logits \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mLinear(out)[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfeatures\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m     35\u001b[0m     logits \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msoftmax(logits)\n",
      "File \u001b[0;32m~/miniforge3/envs/deep_learning/lib/python3.12/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniforge3/envs/deep_learning/lib/python3.12/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/miniforge3/envs/deep_learning/lib/python3.12/site-packages/torch/nn/modules/container.py:219\u001b[0m, in \u001b[0;36mSequential.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    217\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[1;32m    218\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[0;32m--> 219\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    220\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[0;32m~/miniforge3/envs/deep_learning/lib/python3.12/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniforge3/envs/deep_learning/lib/python3.12/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[6], line 106\u001b[0m, in \u001b[0;36mMultiHeadSelfAttentionBlock.forward\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m    104\u001b[0m residual \u001b[38;5;241m=\u001b[39m pooled_values\u001b[38;5;241m.\u001b[39mtranspose(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m3\u001b[39m, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m2\u001b[39m)\u001b[38;5;241m.\u001b[39mreshape(residual\u001b[38;5;241m.\u001b[39mshape)\n\u001b[1;32m    105\u001b[0m residual \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutput_weights(residual)\n\u001b[0;32m--> 106\u001b[0m residual \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdropout\u001b[49m\u001b[43m(\u001b[49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfeatures\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mresidual\u001b[49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfeatures\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m    108\u001b[0m features \u001b[38;5;241m=\u001b[39m skip \u001b[38;5;241m+\u001b[39m residual\n\u001b[1;32m    110\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m batch \u001b[38;5;241m|\u001b[39m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfeatures\u001b[39m\u001b[38;5;124m\"\u001b[39m: features}\n",
      "File \u001b[0;32m~/miniforge3/envs/deep_learning/lib/python3.12/site-packages/torch/nn/modules/module.py:1549\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1546\u001b[0m             tracing_state\u001b[38;5;241m.\u001b[39mpop_scope()\n\u001b[1;32m   1547\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m result\n\u001b[0;32m-> 1549\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_wrapped_call_impl\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m   1550\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   1551\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model = Transformer(config).to(config[\"device\"])\n",
    "optimizer = AdamW(model.parameters())\n",
    "\n",
    "log = train_supervised(\n",
    "    config,\n",
    "    dataset_train,\n",
    "    dataset_valid,\n",
    "    CrossEntropyLoss(),\n",
    "    get_accuracy,\n",
    "    model,\n",
    "    optimizer,\n",
    "    target_key=\"labels\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([667274, 64, 2])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_train['features'].shape"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deep_learning",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
