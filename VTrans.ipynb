{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datasets\n",
    "import transformers\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from datasets import load_dataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch.nn as nn\n",
    "\n",
    "from collections import defaultdict\n",
    "from collections.abc import (\n",
    "    Callable,\n",
    "    Iterable\n",
    ")\n",
    "import numpy as np\n",
    "\n",
    "from util_110724 import (\n",
    "    to_ensembled\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\n",
    "    \"seed\": 0, \n",
    "    \"device\": \"mps\", \n",
    "    \"features_dtype\": torch.float32,\n",
    "    \"hidden_layer_dim\" : 18, \n",
    "    \"ensemble_shape\": (3,5), \n",
    "    \"n_patches\": [2, 4, 8],  # 3 values for n_patches\n",
    "    \"hidden_layer_dim\": [12, 15, 18, 21, 24], # 5 values for hidden layer dimensions ( they are all divisible by 3 = n_head\n",
    "    \"n_heads\": 3\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x169b221d0>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.manual_seed(config[\"seed\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/dh/k6ll1txd3319rq_4r88v0gfm0000gn/T/ipykernel_50278/1535093496.py:1: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  chess_features, chess_labels = torch.load('data/sample_dataset.pt')\n"
     ]
    }
   ],
   "source": [
    "chess_features, chess_labels = torch.load('data/sample_dataset.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_train, features_valid, labels_train, labels_valid = train_test_split(\n",
    "    chess_features, chess_labels, test_size=0.2, random_state=42\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5984 8 8 9\n"
     ]
    }
   ],
   "source": [
    "n, h, w, c = features_train.shape \n",
    "print(n,h,w,c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#patching the images\n",
    "def patchify(images, n_patches):\n",
    "    '''\n",
    "    n is the number of images, \n",
    "    c is the number of channels, in our case it will be 9, \n",
    "    h is the height of the image and w is the width of the image, both be 8 in our case\n",
    "    '''\n",
    "    n, h, w, c = images.shape \n",
    "\n",
    "    assert h == w, \"Patchify method is implemented for square images only\"\n",
    "    \n",
    "    patches = torch.zeros(n, n_patches ** 2, h * w * c// n_patches ** 2)\n",
    "    patch_size = h // n_patches\n",
    "\n",
    "    for idx, image in enumerate(images):\n",
    "        for i in range(n_patches):\n",
    "            for j in range(n_patches):\n",
    "                patch = image[i * patch_size: (i + 1) * patch_size, j * patch_size: (j + 1) * patch_size, :]\n",
    "                patches[idx, i * n_patches + j] = patch.flatten()\n",
    "    return patches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#getting positional embeddings for each token. Here we used sin cos function (work by Vaswani et).\n",
    "def get_positional_embeddings(sequence_length, d):\n",
    "    result = torch.ones(sequence_length, d)\n",
    "    for i in range(sequence_length):\n",
    "        for j in range(d):\n",
    "            result[i][j] = np.sin(i / (10000 ** (j / d))) if j % 2 == 0 else np.cos(i / (10000 ** ((j - 1) / d)))\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Idea for two move: encode can_move element of d vector to 0 (restricting movement to only piece selected)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "class PreViT(nn.Module):\n",
    "  \"Here we have initialization of the model and patching\"\n",
    "  def __init__(self, chw=(9, 8, 8), n_patches=4, hidden_layer_dim = 18):\n",
    "    # Super constructor\n",
    "    super(PreViT, self).__init__()\n",
    "\n",
    "    # Attributes\n",
    "    self.chw = chw # (C, H, W)\n",
    "    self.n_patches = n_patches\n",
    "    self.hidden_layer_dim = hidden_layer_dim\n",
    "\n",
    "    assert chw[1] % n_patches == 0, \"Input shape not entirely divisible by number of patches\"\n",
    "    assert chw[2] % n_patches == 0, \"Input shape not entirely divisible by number of patches\"\n",
    "    \n",
    "    self.patch_size = (chw[1] / n_patches, chw[2] / n_patches)\n",
    "    \n",
    "    # mapping to a linear vector\n",
    "    self.input_vector_dim = int(chw[0] * self.patch_size[0] * self.patch_size[1])\n",
    "    self.linear_mapper = nn.Linear(self.input_vector_dim, self.hidden_layer_dim)\n",
    "    \n",
    "    # create a classification token\n",
    "    self.class_token = nn.Parameter(torch.rand(1, self.hidden_layer_dim))\n",
    "    \n",
    "    \n",
    "  def forward(self, images):\n",
    "    patches = patchify(images, self.n_patches)\n",
    "    tokens = self.linear_mapper(patches)\n",
    "    \n",
    "    tokens = torch.stack([torch.vstack((self.class_token, tokens[i])) for i in range(len(tokens))])\n",
    "    \n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have two hyperparameters for processing the data, \"n_patches\" and \"hidden_layer_dimension\", we will attempt to try different combinations of them using ensemble. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PreViT(nn.Module):\n",
    "  \"Here we have initialization of the model and patching\"\n",
    "  def __init__(self, chw=(9, 8, 8), config = None):\n",
    "    # Super constructor\n",
    "    super(PreViT, self).__init__()\n",
    "\n",
    "    assert config is not None, \"Config must provide ensemble shape, n_patches, and hidden_layer_dim\"\n",
    "    # Attributes\n",
    "    self.chw = chw # (C, H, W)\n",
    "    self.ensemble_shape = config[\"ensemble_shape\"]\n",
    "    self.n_patches_values = config[\"n_patches\"]  # List of 3 values for n_patches\n",
    "    self.hidden_layer_dims = config[\"hidden_layer_dim\"]\n",
    "\n",
    "    assert len(self.n_patches_values) == self.ensemble_shape[0], \"n_patches must have 3 values\"\n",
    "    assert len(self.hidden_layer_dims) == self.ensemble_shape[1], \"hidden_layer_dim must have 5 values\"\n",
    "\n",
    "    for n_patches in self.n_patches_values:\n",
    "      assert chw[1] % n_patches == 0, f\"Height {chw[1]} is not divisible by n_patches={n_patches}\"\n",
    "      assert chw[2] % n_patches == 0, f\"Width {chw[2]} is not divisible by n_patches={n_patches}\"\n",
    "    \n",
    "    self.patch_size = (chw[1] / n_patches, chw[2] / n_patches)\n",
    "\n",
    "    # Patching and Linear Mapping (to a vector of hidden_dim) \"Tokenize\"\n",
    "    self.linear_mappers = nn.ModuleList([\n",
    "            nn.ModuleList([\n",
    "                nn.Linear(\n",
    "                    int(chw[0] * (chw[1] / n_patches) * (chw[2] / n_patches)), hidden_dim\n",
    "                )\n",
    "                for hidden_dim in self.hidden_layer_dims\n",
    "            ])\n",
    "            for n_patches in self.n_patches_values\n",
    "        ])\n",
    "    \n",
    "    # Add the special token for the start of each block\n",
    "    self.class_tokens = nn.ParameterList([\n",
    "            nn.ParameterList([\n",
    "                nn.Parameter(torch.rand(1, hidden_dim))\n",
    "                for hidden_dim in self.hidden_layer_dims\n",
    "            ])\n",
    "            for _ in self.n_patches_values\n",
    "        ])\n",
    "    \n",
    "    # Add Positional Embeddings \n",
    "    self.pos_embeddings = nn.ParameterList([\n",
    "    nn.ParameterList([\n",
    "        nn.Parameter(\n",
    "            torch.tensor(get_positional_embeddings(n_patches ** 2 + 1, hidden_dim))\n",
    "        )\n",
    "        for hidden_dim in self.hidden_layer_dims\n",
    "    ])\n",
    "    for n_patches in self.n_patches_values\n",
    "])\n",
    "    # Make sure that the Positional Embeddings are not learnable. \n",
    "    for param_list in self.pos_embeddings:\n",
    "        for param in param_list:\n",
    "            param.requires_grad = False\n",
    "    \n",
    "  def forward(self, images, ensemble_idx):\n",
    "        \"\"\"\n",
    "            images: Input images of shape (# of games, C, H, W)\n",
    "            ensemble_idx: A tuple (n_patches_idx, hidden_layer_dim_idx) indicating which ensemble configuration to use\n",
    "        \"\"\"\n",
    "        n_patches_idx, hidden_layer_dim_idx = ensemble_idx\n",
    "\n",
    "        # Select the configuration\n",
    "        n_patches = self.n_patches_values[n_patches_idx]\n",
    "        linear_mapper = self.linear_mappers[n_patches_idx][hidden_layer_dim_idx]\n",
    "        class_token = self.class_tokens[n_patches_idx][hidden_layer_dim_idx]\n",
    "        pos_embedding = self.pos_embeddings[n_patches_idx][hidden_layer_dim_idx]\n",
    "\n",
    "        # Patching the inputs\n",
    "        patches = patchify(images, n_patches)  \n",
    "\n",
    "        # Apply the linear mapper to the patches\n",
    "        tokens = linear_mapper(patches)\n",
    "\n",
    "        # Add the classification token\n",
    "        tokens = torch.stack([torch.vstack((class_token, tokens[i])) for i in range(len(tokens))])\n",
    "\n",
    "        # Add Positional Embeddings\n",
    "        positional_embed = pos_embedding.repeat(n, 1, 1)\n",
    "        out = tokens + positional_embed\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/dh/k6ll1txd3319rq_4r88v0gfm0000gn/T/ipykernel_50278/1145193790.py:47: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  torch.tensor(get_positional_embeddings(n_patches ** 2 + 1, hidden_dim))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([5984, 17, 18])\n"
     ]
    }
   ],
   "source": [
    "model = PreViT(chw=(9, 8, 8), config=config)\n",
    "ensemble_idx = (1, 2)  # Second n_patches (4), third hidden_layer_dim (18) (indexes hyperparameters)\n",
    "tokens = model(features_train, ensemble_idx)\n",
    "print(tokens.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(config[\"ensemble_shape\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Layer Normalziation which allows different hyperparameters to be applied per ensemble dimension or configuration. \n",
    "class LayerNorm(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        config: dict,\n",
    "        normalized_shape: int | tuple[int],\n",
    "        bias=True,\n",
    "        elementwise_affine=True,\n",
    "        epsilon=1e-5,\n",
    "        normalized_offset=0\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        if hasattr(normalized_shape, \"__int__\"):\n",
    "            self.normalized_shape = (normalized_shape,)\n",
    "        else:\n",
    "            self.normalized_shape = normalized_shape\n",
    "\n",
    "        self.ensemble_shape = config[\"ensemble_shape\"]\n",
    "        self.epsilon = epsilon\n",
    "        self.normalized_offset = normalized_offset\n",
    "\n",
    "        if elementwise_affine:\n",
    "            self.scale = torch.nn.Parameter(torch.ones(\n",
    "                self.ensemble_shape + self.normalized_shape + (1,) * normalized_offset,\n",
    "                device=config[\"device\"],\n",
    "                dtype=config[\"features_dtype\"]\n",
    "            ))\n",
    "            if bias:\n",
    "                self.bias = torch.nn.Parameter(torch.zeros_like(self.scale))\n",
    "            else:\n",
    "                self.bias = None\n",
    "\n",
    "        else:\n",
    "            self.bias, self.scale = None, None\n",
    "\n",
    "\n",
    "    def forward(self, features: dict) -> dict:\n",
    "\n",
    "        ensemble_dim = len(self.ensemble_shape)\n",
    "        features = to_ensembled(self.ensemble_shape, features)\n",
    "\n",
    "        normalized_dim = len(self.normalized_shape)\n",
    "\n",
    "        batch_dim = len(features.shape) - ensemble_dim - normalized_dim - self.normalized_offset\n",
    "        normalized_range = tuple(range(\n",
    "            ensemble_dim,\n",
    "            ensemble_dim + batch_dim\n",
    "        )) + tuple(range(\n",
    "            -normalized_dim - self.normalized_offset,\n",
    "            -self.normalized_offset\n",
    "        ))\n",
    "\n",
    "        features = features - features.mean(dim=normalized_range, keepdim=True)\n",
    "        features = features / features.std(dim=normalized_range, keepdim=True)\n",
    "\n",
    "        if self.scale is not None:\n",
    "            scale = self.scale.unflatten(\n",
    "                ensemble_dim,\n",
    "                (1,) * batch_dim + self.normalized_shape[:1]\n",
    "            )\n",
    "\n",
    "            features = features * scale\n",
    "\n",
    "            if self.bias is not None:\n",
    "                bias = self.bias.unflatten(\n",
    "                    ensemble_dim,\n",
    "                    (1,) * batch_dim + self.normalized_shape[:1]\n",
    "                )\n",
    "                features = features + bias\n",
    "\n",
    "        return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here we apply multi-head self-attention to the treated tokens (input here has shape (N, 17, 18 = d))\n",
    "class MyMSA(nn.Module):\n",
    "    def __init__(self, config = None):\n",
    "        super(MyMSA, self).__init__()\n",
    "\n",
    "        assert config is not None, \"Config dictionary must be provided\"\n",
    "\n",
    "        self.ensemble_shape = config[\"ensemble_shape\"] \n",
    "        self.hidden_layer_dims = config[\"hidden_layer_dim\"]\n",
    "        self.n_heads = config[\"n_heads\"]\n",
    "\n",
    "        assert self.hidden_layer_dims % self.n_heads == 0, f\"Can't divide dimension {self.hidden_layer_dims} into {self.n_heads} heads\" #checker function make sure the n_head is good\n",
    "\n",
    "        self.ensemble_shape = config[\"ensemble_shape\"]\n",
    "\n",
    "            \n",
    "        #creating que, key, and value mappings.\n",
    "        self.q_mappings = nn.ModuleList([\n",
    "            nn.ModuleList([nn.Linear(int(self.hidden_layer_dims[i] / self.n_heads), int(self.hidden_layer_dims[i] / self.n_heads)) for _ in range(self.n_heads)])\n",
    "            for i in range(self.ensemble_shape[1])  # Create a separate set of heads per ensemble\n",
    "        ])\n",
    "\n",
    "        self.k_mappings = nn.ModuleList([\n",
    "            nn.ModuleList([nn.Linear(int(self.hidden_layer_dims[i] / self.n_heads), int(self.hidden_layer_dims[i] / self.n_heads)) for _ in range(self.n_heads)])\n",
    "            for i in range(self.ensemble_shape[1])  # Create a separate set of heads per ensemble\n",
    "        ])\n",
    "\n",
    "        self.v_mappings = nn.ModuleList([\n",
    "            nn.ModuleList([nn.Linear(int(self.hidden_layer_dims[i] / self.n_heads), int(self.hidden_layer_dims[i] / self.n_heads)) for _ in range(self.n_heads)])\n",
    "            for i in range(self.ensemble_shape[1])  # Create a separate set of heads per ensemble\n",
    "        ])\n",
    "\n",
    "        self.softmax = nn.Softmax(dim=-1)\n",
    "\n",
    "    def forward(self, sequences, ensemble_idx):\n",
    "        # Sequences has shape (N, seq_length, token_dim)\n",
    "        # Ensemble_idx has shape tuple (n_patches_idx, hidden_layer_dim_idx)\n",
    "        # We go into shape    (N, seq_length, n_heads, token_dim / n_heads)\n",
    "        # And come back to    (N, seq_length, item_dim)  (through concatenation)\n",
    "        n_patches_idx, hidden_layer_dim_idx = ensemble_idx\n",
    "\n",
    "        self.d_head = int(self.hidden_layer_dims[hidden_layer_dim_idx] / self.n_heads)\n",
    "\n",
    "        result = []\n",
    "        q_mapping_idx = self.q_mappings[hidden_layer_dim_idx]\n",
    "        k_mapping_idx = self.k_mappings[hidden_layer_dim_idx]\n",
    "        v_mapping_idx = self.v_mappings[hidden_layer_dim_idx]\n",
    "        for sequence in sequences:\n",
    "            seq_result = []\n",
    "            for head in range(self.n_heads):\n",
    "                q_mapping = q_mapping_idx[head]\n",
    "                k_mapping = k_mapping_idx[head]\n",
    "                v_mapping = v_mapping_idx[head]\n",
    "\n",
    "                seq = sequence[:, head * self.d_head: (head + 1) * self.d_head]\n",
    "                q, k, v = q_mapping(seq), k_mapping(seq), v_mapping(seq)\n",
    "\n",
    "                attention = self.softmax(q @ k.T / (self.d_head ** 0.5))\n",
    "                seq_result.append(attention @ v)\n",
    "            result.append(torch.hstack(seq_result))\n",
    "        return torch.cat([torch.unsqueeze(r, dim=0) for r in result])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Having several heads per layer is similar to having several kernels in convolution.\n",
    "\n",
    "Having several heads per layer allows one model to try out several pathways at once."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(\n",
    "    {\n",
    "        \"train_features\": features_train,\n",
    "        \"train_labels\": labels_train,\n",
    "        \"valid_features\": features_valid,\n",
    "        \"valid_labels\": labels_valid\n",
    "    },\n",
    "    \"preprocessed_train_valid_data\"\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DML",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
