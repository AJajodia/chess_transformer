{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_LAUNCH_BLOCKING\"] = \"1\"\n",
    "# import datasets\n",
    "# import transformers\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from tqdm.notebook import tqdm, trange\n",
    "from torch.optim import Adam\n",
    "from torch.nn import CrossEntropyLoss\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "from torchvision.transforms import ToTensor\n",
    "\n",
    "# from datasets import load_dataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch.nn as nn\n",
    "\n",
    "from collections import defaultdict\n",
    "from collections.abc import (\n",
    "    Callable,\n",
    "    Iterable\n",
    ")\n",
    "import numpy as np\n",
    "\n",
    "# from util_110724 import (\n",
    "#     to_ensembled\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\n",
    "    \"seed\": 0, \n",
    "    \"device\": \"cuda\", \n",
    "    \"features_dtype\": torch.long,\n",
    "    \"ensemble_shape\": (3,5), \n",
    "    \"n_patches\": [2, 4, 8],  # 3 values for n_patches\n",
    "    \"hidden_layer_dim\": [12, 15, 18, 21, 24], # 5 values for hidden layer dimensions ( they are all divisible by 3 = n_head\n",
    "    \"n_heads\": 3\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0xec7b190>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.manual_seed(config[\"seed\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Anu\\AppData\\Local\\Temp\\ipykernel_5556\\1535093496.py:1: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  chess_features, chess_labels = torch.load('data/sample_dataset.pt')\n"
     ]
    }
   ],
   "source": [
    "chess_features, chess_labels = torch.load('data/sample_dataset.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_train, features_valid, labels_train, labels_valid = train_test_split(\n",
    "    chess_features, chess_labels, test_size=0.2, random_state=42\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can write a function to check the number of classification classes a set of board games have\n",
    "def get_out_d(lables): \n",
    "    unique_pairs = torch.unique(lables, dim=0)  # Find unique rows\n",
    "    num_unique_pairs = unique_pairs.size(0)\n",
    "\n",
    "    return num_unique_pairs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#patching the images\n",
    "def patchify(images, n_patches):\n",
    "    '''\n",
    "    n is the number of images, \n",
    "    c is the number of channels, in our case it will be 9, \n",
    "    h is the height of the image and w is the width of the image, both be 8 in our case\n",
    "    '''\n",
    "    n, h, w, c = images.shape \n",
    "\n",
    "    assert h == w, \"Patchify method is implemented for square images only\"\n",
    "    \n",
    "    patches = torch.zeros(n, n_patches ** 2, h * w * c// n_patches ** 2, device=config['device'])\n",
    "    patch_size = h // n_patches\n",
    "\n",
    "    for idx, image in enumerate(images):\n",
    "        for i in range(n_patches):\n",
    "            for j in range(n_patches):\n",
    "                patch = image[i * patch_size: (i + 1) * patch_size, j * patch_size: (j + 1) * patch_size, :]\n",
    "                patches[idx, i * n_patches + j] = patch.flatten()\n",
    "    return patches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#getting positional embeddings for each token. Here we used sin cos function (work by Vaswani et).\n",
    "def get_positional_embeddings(sequence_length, d):\n",
    "    result = torch.ones(sequence_length, d, device=config[\"device\"])\n",
    "    for i in range(sequence_length):\n",
    "        for j in range(d):\n",
    "            result[i][j] = np.sin(i / (10000 ** (j / d))) if j % 2 == 0 else np.cos(i / (10000 ** ((j - 1) / d)))\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Idea for two move: encode can_move element of d vector to 0 (restricting movement to only piece selected)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "class PreViT(nn.Module):\n",
    "  \"Here we have initialization of the model and patching\"\n",
    "  def __init__(self, chw=(9, 8, 8), n_patches=4, hidden_layer_dim = 18):\n",
    "    # Super constructor\n",
    "    super(PreViT, self).__init__()\n",
    "\n",
    "    # Attributes\n",
    "    self.chw = chw # (C, H, W)\n",
    "    self.n_patches = n_patches\n",
    "    self.hidden_layer_dim = hidden_layer_dim\n",
    "\n",
    "    assert chw[1] % n_patches == 0, \"Input shape not entirely divisible by number of patches\"\n",
    "    assert chw[2] % n_patches == 0, \"Input shape not entirely divisible by number of patches\"\n",
    "    \n",
    "    self.patch_size = (chw[1] / n_patches, chw[2] / n_patches)\n",
    "    \n",
    "    # mapping to a linear vector\n",
    "    self.input_vector_dim = int(chw[0] * self.patch_size[0] * self.patch_size[1])\n",
    "    self.linear_mapper = nn.Linear(self.input_vector_dim, self.hidden_layer_dim)\n",
    "    \n",
    "    # create a classification token\n",
    "    self.class_token = nn.Parameter(torch.rand(1, self.hidden_layer_dim))\n",
    "    \n",
    "    \n",
    "  def forward(self, images):\n",
    "    patches = patchify(images, self.n_patches)\n",
    "    tokens = self.linear_mapper(patches)\n",
    "    \n",
    "    tokens = torch.stack([torch.vstack((self.class_token, tokens[i])) for i in range(len(tokens))])\n",
    "    \n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have two hyperparameters for processing the data, \"n_patches\" and \"hidden_layer_dimension\", we will attempt to try different combinations of them using ensemble. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PreViT(nn.Module):\n",
    "  \"Here we have initialization of the model and patching\"\n",
    "  def __init__(self, chw=(9, 8, 8), config = None):\n",
    "    # Super constructor\n",
    "    super(PreViT, self).__init__()\n",
    "\n",
    "    assert config is not None, \"Config must provide ensemble shape, n_patches, and hidden_layer_dim\"\n",
    "    # Attributes\n",
    "    self.chw = chw # (C, H, W)\n",
    "    self.ensemble_shape = config[\"ensemble_shape\"]\n",
    "    self.n_patches_values = config[\"n_patches\"]  # List of 3 values for n_patches\n",
    "    self.hidden_layer_dims = config[\"hidden_layer_dim\"]\n",
    "\n",
    "    assert len(self.n_patches_values) == self.ensemble_shape[0], \"n_patches must have 3 values\"\n",
    "    assert len(self.hidden_layer_dims) == self.ensemble_shape[1], \"hidden_layer_dim must have 5 values\"\n",
    "\n",
    "    for n_patches in self.n_patches_values:\n",
    "      assert chw[1] % n_patches == 0, f\"Height {chw[1]} is not divisible by n_patches={n_patches}\"\n",
    "      assert chw[2] % n_patches == 0, f\"Width {chw[2]} is not divisible by n_patches={n_patches}\"\n",
    "    \n",
    "    self.patch_size = (chw[1] / n_patches, chw[2] / n_patches)\n",
    "\n",
    "    # Patching and Linear Mapping (to a vector of hidden_dim) \"Tokenize\"\n",
    "    self.linear_mappers = nn.ModuleList([\n",
    "            nn.ModuleList([\n",
    "                nn.Linear(\n",
    "                    int(chw[0] * (chw[1] / n_patches) * (chw[2] / n_patches)), hidden_dim\n",
    "                )\n",
    "                for hidden_dim in self.hidden_layer_dims\n",
    "            ])\n",
    "            for n_patches in self.n_patches_values\n",
    "        ])\n",
    "    \n",
    "    # Add the special token for the start of each block\n",
    "    self.class_tokens = nn.ParameterList([\n",
    "            nn.ParameterList([\n",
    "                nn.Parameter(torch.rand(1, hidden_dim))\n",
    "                for hidden_dim in self.hidden_layer_dims\n",
    "            ])\n",
    "            for _ in self.n_patches_values\n",
    "        ])\n",
    "    \n",
    "    # Add Positional Embeddings \n",
    "    self.pos_embeddings = nn.ParameterList([\n",
    "    nn.ParameterList([\n",
    "        nn.Parameter(\n",
    "            torch.tensor(get_positional_embeddings(n_patches ** 2 + 1, hidden_dim))\n",
    "        )\n",
    "        for hidden_dim in self.hidden_layer_dims\n",
    "    ])\n",
    "    for n_patches in self.n_patches_values\n",
    "])\n",
    "    # Make sure that the Positional Embeddings are not learnable. \n",
    "    for param_list in self.pos_embeddings:\n",
    "        for param in param_list:\n",
    "            param.requires_grad = False\n",
    "    \n",
    "  def forward(self, images, ensemble_idx):\n",
    "        \"\"\"\n",
    "            images: Input images of shape (# of games, C, H, W)\n",
    "            ensemble_idx: A tuple (n_patches_idx, hidden_layer_dim_idx) indicating which ensemble configuration to use\n",
    "        \"\"\"\n",
    "        n_patches_idx, hidden_layer_dim_idx = ensemble_idx\n",
    "        n = images.shape[0]\n",
    "\n",
    "        # Select the configuration\n",
    "        n_patches = self.n_patches_values[n_patches_idx]\n",
    "        linear_mapper = self.linear_mappers[n_patches_idx][hidden_layer_dim_idx]\n",
    "        class_token = self.class_tokens[n_patches_idx][hidden_layer_dim_idx]\n",
    "        pos_embedding = self.pos_embeddings[n_patches_idx][hidden_layer_dim_idx]\n",
    "\n",
    "        # Patching the inputs\n",
    "        patches = patchify(images, n_patches)  \n",
    "\n",
    "        # Apply the linear mapper to the patches\n",
    "        tokens = linear_mapper(patches)\n",
    "\n",
    "        # Add the classification token\n",
    "        tokens = torch.stack([torch.vstack((class_token, tokens[i])) for i in range(len(tokens))])\n",
    "\n",
    "        # Add Positional Embeddings\n",
    "        positional_embed = pos_embedding.repeat(n, 1, 1)\n",
    "        out = tokens + positional_embed\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #Layer Normalziation which allows different hyperparameters to be applied per ensemble dimension or configuration. \n",
    "# class LayerNorm(nn.Module):\n",
    "#     def __init__(\n",
    "#         self,\n",
    "#         config: dict,\n",
    "#         normalized_shape: int | tuple[int],\n",
    "#         bias=True,\n",
    "#         elementwise_affine=True,\n",
    "#         epsilon=1e-5,\n",
    "#         normalized_offset=0\n",
    "#     ):\n",
    "#         super().__init__()\n",
    "\n",
    "#         if hasattr(normalized_shape, \"__int__\"):\n",
    "#             self.normalized_shape = (normalized_shape,)\n",
    "#         else:\n",
    "#             self.normalized_shape = normalized_shape\n",
    "\n",
    "#         self.ensemble_shape = config[\"ensemble_shape\"]\n",
    "#         self.epsilon = epsilon\n",
    "#         self.normalized_offset = normalized_offset\n",
    "\n",
    "#         if elementwise_affine:\n",
    "#             self.scale = torch.nn.Parameter(torch.ones(\n",
    "#                 self.ensemble_shape + self.normalized_shape + (1,) * normalized_offset,\n",
    "#                 device=config[\"device\"],\n",
    "#                 dtype=config[\"features_dtype\"]\n",
    "#             ))\n",
    "#             if bias:\n",
    "#                 self.bias = torch.nn.Parameter(torch.zeros_like(self.scale))\n",
    "#             else:\n",
    "#                 self.bias = None\n",
    "\n",
    "#         else:\n",
    "#             self.bias, self.scale = None, None\n",
    "\n",
    "\n",
    "#     def forward(self, features: dict) -> dict:\n",
    "\n",
    "#         ensemble_dim = len(self.ensemble_shape)\n",
    "#         features = to_ensembled(self.ensemble_shape, features)\n",
    "\n",
    "#         normalized_dim = len(self.normalized_shape)\n",
    "\n",
    "#         batch_dim = len(features.shape) - ensemble_dim - normalized_dim - self.normalized_offset\n",
    "#         normalized_range = tuple(range(\n",
    "#             ensemble_dim,\n",
    "#             ensemble_dim + batch_dim\n",
    "#         )) + tuple(range(\n",
    "#             -normalized_dim - self.normalized_offset,\n",
    "#             -self.normalized_offset\n",
    "#         ))\n",
    "\n",
    "#         features = features - features.mean(dim=normalized_range, keepdim=True)\n",
    "#         features = features / features.std(dim=normalized_range, keepdim=True)\n",
    "\n",
    "#         if self.scale is not None:\n",
    "#             scale = self.scale.unflatten(\n",
    "#                 ensemble_dim,\n",
    "#                 (1,) * batch_dim + self.normalized_shape[:1]\n",
    "#             )\n",
    "\n",
    "#             features = features * scale\n",
    "\n",
    "#             if self.bias is not None:\n",
    "#                 bias = self.bias.unflatten(\n",
    "#                     ensemble_dim,\n",
    "#                     (1,) * batch_dim + self.normalized_shape[:1]\n",
    "#                 )\n",
    "#                 features = features + bias\n",
    "\n",
    "#         return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here we apply multi-head self-attention to the treated tokens (input here has shape (N, 17, 18 = d))\n",
    "class MyMSA(nn.Module):\n",
    "    def __init__(self, config = None):\n",
    "        super(MyMSA, self).__init__()\n",
    "\n",
    "        assert config is not None, \"Config dictionary must be provided\"\n",
    "\n",
    "        self.ensemble_shape = config[\"ensemble_shape\"] \n",
    "        self.hidden_layer_dims = config[\"hidden_layer_dim\"]\n",
    "        self.n_heads = config[\"n_heads\"]\n",
    "\n",
    "        # Assert that every hidden_layer_dim is divisible by n_heads\n",
    "        for hidden_dim in self.hidden_layer_dims:\n",
    "            assert hidden_dim % self.n_heads == 0, f\"Can't divide dimension {hidden_dim} into {self.n_heads} heads\"\n",
    "\n",
    "        self.ensemble_shape = config[\"ensemble_shape\"]\n",
    "\n",
    "            \n",
    "        #creating que, key, and value mappings.\n",
    "        self.q_mappings = nn.ModuleList([\n",
    "            nn.ModuleList([nn.Linear(int(self.hidden_layer_dims[i] / self.n_heads), int(self.hidden_layer_dims[i] / self.n_heads)) for _ in range(self.n_heads)])\n",
    "            for i in range(self.ensemble_shape[1])  # Create a separate set of heads per ensemble\n",
    "        ])\n",
    "\n",
    "        self.k_mappings = nn.ModuleList([\n",
    "            nn.ModuleList([nn.Linear(int(self.hidden_layer_dims[i] / self.n_heads), int(self.hidden_layer_dims[i] / self.n_heads)) for _ in range(self.n_heads)])\n",
    "            for i in range(self.ensemble_shape[1])  # Create a separate set of heads per ensemble\n",
    "        ])\n",
    "\n",
    "        self.v_mappings = nn.ModuleList([\n",
    "            nn.ModuleList([nn.Linear(int(self.hidden_layer_dims[i] / self.n_heads), int(self.hidden_layer_dims[i] / self.n_heads)) for _ in range(self.n_heads)])\n",
    "            for i in range(self.ensemble_shape[1])  # Create a separate set of heads per ensemble\n",
    "        ])\n",
    "\n",
    "        self.softmax = nn.Softmax(dim=-1)\n",
    "\n",
    "    def forward(self, sequences, ensemble_idx):\n",
    "        # Sequences has shape (N, seq_length, token_dim)\n",
    "        # Ensemble_idx has shape tuple (n_patches_idx, hidden_layer_dim_idx)\n",
    "        # We go into shape    (N, seq_length, n_heads, token_dim / n_heads)\n",
    "        # And come back to    (N, seq_length, item_dim)  (through concatenation)\n",
    "        n_patches_idx, hidden_layer_dim_idx = ensemble_idx\n",
    "\n",
    "        self.d_head = int(self.hidden_layer_dims[hidden_layer_dim_idx] / self.n_heads)\n",
    "\n",
    "        result = []\n",
    "        q_mapping_idx = self.q_mappings[hidden_layer_dim_idx]\n",
    "        k_mapping_idx = self.k_mappings[hidden_layer_dim_idx]\n",
    "        v_mapping_idx = self.v_mappings[hidden_layer_dim_idx]\n",
    "        for sequence in sequences:\n",
    "            seq_result = []\n",
    "            for head in range(self.n_heads):\n",
    "                q_mapping = q_mapping_idx[head]\n",
    "                k_mapping = k_mapping_idx[head]\n",
    "                v_mapping = v_mapping_idx[head]\n",
    "\n",
    "                seq = sequence[:, head * self.d_head: (head + 1) * self.d_head]\n",
    "                q, k, v = q_mapping(seq), k_mapping(seq), v_mapping(seq)\n",
    "\n",
    "                attention = self.softmax(q @ k.T / (self.d_head ** 0.5))\n",
    "                seq_result.append(attention @ v)\n",
    "            result.append(torch.hstack(seq_result))\n",
    "        return torch.cat([torch.unsqueeze(r, dim=0) for r in result])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyViTBlock(nn.Module):\n",
    "    def __init__(self, mlp_ratio=4, config = None):\n",
    "        super(MyViTBlock, self).__init__()\n",
    "\n",
    "        assert config is not None, \"Config dictionary must be provided\"\n",
    "        \n",
    "        self.hidden_layer_dims = config[\"hidden_layer_dim\"]\n",
    "        self.n_heads = config[\"n_heads\"]\n",
    "        self.n_patches = config[\"n_patches\"]\n",
    "        self.ensemble_shape = config[\"ensemble_shape\"]\n",
    "\n",
    "        self.norm1 = nn.ModuleList([\n",
    "            nn.LayerNorm(hidden_dim)\n",
    "            for hidden_dim in self.hidden_layer_dims\n",
    "        ])\n",
    "        self.mhsa = MyMSA(config)\n",
    "        self.norm2 = nn.ModuleList([\n",
    "            nn.LayerNorm(hidden_dim)\n",
    "            for hidden_dim in self.hidden_layer_dims\n",
    "        ])\n",
    "\n",
    "        self.dropout = nn.Dropout(.5)\n",
    "        self.mlp = nn.ModuleList([\n",
    "                nn.Sequential(\n",
    "                    nn.Linear(hidden_dim, mlp_ratio* hidden_dim),  # MLP ratio fixed at 4\n",
    "                    nn.GELU(),\n",
    "                    nn.Linear(4 * hidden_dim, hidden_dim),\n",
    "                )\n",
    "            for hidden_dim in self.hidden_layer_dims\n",
    "        ])\n",
    "\n",
    "    def forward(self, x, ensemble_idx):\n",
    "        n_patches_idx, hidden_layer_dim_idx = ensemble_idx\n",
    "        norm1 = self.norm1[hidden_layer_dim_idx]\n",
    "        norm2 = self.norm2[hidden_layer_dim_idx]\n",
    "        mlp = self.mlp[hidden_layer_dim_idx]\n",
    "        \n",
    "        # Apply LayerNorm\n",
    "        x_norm1 = norm1(x)\n",
    "\n",
    "        # Multi-Head Self-Attention\n",
    "        x_mhsa = self.mhsa(x_norm1, ensemble_idx)\n",
    "        \n",
    "        # Residual connection\n",
    "        out = x + x_mhsa\n",
    "\n",
    "        # Apply second LayerNorm\n",
    "        x_norm2 = norm2(out)\n",
    "\n",
    "        x_norm2 = self.dropout(x_norm2)\n",
    "\n",
    "        # Apply MLP\n",
    "        x_mlp = mlp(x_norm2)\n",
    "\n",
    "        # Second residual connection\n",
    "        out = out + x_mlp\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_blocks = 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Having several heads per layer is similar to having several kernels in convolution.\n",
    "\n",
    "Having several heads per layer allows one model to try out several pathways at once."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# n_blocks is the number of transformer blocks that this model want to include. here we will train with 3 blocks. \n",
    "\n",
    "class MyViT(nn.Module):\n",
    "    def __init__(self, chw, config, ensemble_idx, labels, n_blocks=3, n_heads=3):\n",
    "        # Super constructor\n",
    "        super(MyViT, self).__init__()\n",
    "        \n",
    "        # Attributes\n",
    "        self.chw = chw # ( C , H , W )\n",
    "        self.config = config\n",
    "        self.ensemble_idx = ensemble_idx\n",
    "        self.n_blocks = n_blocks\n",
    "        self.n_heads = n_heads\n",
    "        self.hidden_layer_dims = config[\"hidden_layer_dim\"]\n",
    "\n",
    "        n_patches_idx, hidden_layer_dim_idx = ensemble_idx\n",
    "\n",
    "        # Retrieve hidden_dim based on the ensemble configuration\n",
    "        self.hidden_d = self.hidden_layer_dims[hidden_layer_dim_idx]\n",
    "\n",
    "        # Dynamically determine the number of output classes\n",
    "        self.out_d = get_out_d(labels)\n",
    "        \n",
    "        # 1) PreViT: Prepare Data (Tokenization, Positional Embeddings, and Classification Token)\n",
    "        self.previt = PreViT(chw=chw, config=config)\n",
    "        \n",
    "        # 2) Transformer Encoder Blocks\n",
    "        self.blocks = nn.ModuleList([\n",
    "            MyViTBlock(config = config) for _ in range(n_blocks)\n",
    "        ])\n",
    "        \n",
    "        # 3) Classification MLP\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(self.hidden_d, self.out_d),\n",
    "            nn.Softmax(dim=-1)\n",
    "        )\n",
    "\n",
    "    def forward(self, images):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            images: Input tensor of shape (batch_size, C, H, W)\n",
    "        \"\"\"\n",
    "        # PreViT: Tokenize input and add positional embeddings\n",
    "        tokens = self.previt(images, self.ensemble_idx)\n",
    "        \n",
    "        # Transformer Blocks: Process the tokens\n",
    "        for block in self.blocks:\n",
    "            tokens = block(tokens, self.ensemble_idx)\n",
    "\n",
    "        # Classification Token: Take the first token\n",
    "        cls_token = tokens[:, 0]\n",
    "        \n",
    "        return self.mlp(cls_token) # Map to output dimension, output category distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Initialize the MyViT models\n",
    "# config = {\n",
    "#     \"seed\": 0, \n",
    "#     \"device\": \"mps\", \n",
    "#     \"features_dtype\": torch.float32,\n",
    "#     \"hidden_layer_dim\" : 18, \n",
    "#     \"ensemble_shape\": (3,5), \n",
    "#     \"n_patches\": [2, 4, 8],  # 3 values for n_patches\n",
    "#     \"hidden_layer_dim\": [12, 15, 18, 21, 24], # 5 values for hidden layer dimensions ( they are all divisible by 3 = n_head\n",
    "#     \"n_heads\": 3\n",
    "# }\n",
    "# ensemble_idx = (1,2)\n",
    "# chw = (9,8,8)\n",
    "# model = MyViT(chw=chw, ensemble_idx=ensemble_idx, labels=labels_train, n_blocks=3, n_heads=3, config = config)\n",
    "\n",
    "# # Forward pass\n",
    "# output = model(features_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# config = {\n",
    "#     \"seed\": 0, \n",
    "#     \"device\": \"mps\", \n",
    "#     \"features_dtype\": torch.float32,\n",
    "#     \"ensemble_shape\": (3,5), \n",
    "#     \"n_patches\": [2, 4, 8],  # 3 values for n_patches\n",
    "#     \"hidden_layer_dim\": [12, 15, 30, 21, 24], # 5 values for hidden layer dimensions ( they are all divisible by 3 = n_head\n",
    "#     \"n_heads\": 3\n",
    "# }\n",
    "# # Iterate over all combinations of the ensemble configurations\n",
    "# for n_patches_idx in range(config[\"ensemble_shape\"][0]):  \n",
    "#     for hidden_layer_dim_idx in range(config[\"ensemble_shape\"][1]):  \n",
    "#         # Define ensemble index\n",
    "#         ensemble_idx = (n_patches_idx, hidden_layer_dim_idx)\n",
    "#         chw = (9, 8, 8)\n",
    "#         # Initialize the model for the current ensemble configuration\n",
    "#         model = MyViT(\n",
    "#             chw=chw,\n",
    "#             config=config,\n",
    "#             ensemble_idx=ensemble_idx,\n",
    "#             labels=labels_train,\n",
    "#             n_blocks=3,\n",
    "#             n_heads=3\n",
    "#         )\n",
    "        \n",
    "#         # Forward pass\n",
    "#         output = model(features_train)\n",
    "#         print(output.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Anu\\AppData\\Local\\Temp\\ipykernel_5556\\2871294209.py:11: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  chess_features, chess_labels = torch.load('data/sample_dataset.pt')\n",
      "C:\\Users\\Anu\\AppData\\Local\\Temp\\ipykernel_5556\\1946074777.py:47: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  torch.tensor(get_positional_embeddings(n_patches ** 2 + 1, hidden_dim))\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "87c4ab9768734545a1383cb94d0bf6ad",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "99b749d1d88042078c46e5c92fe3c7b1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 1 in training:   0%|          | 0/12 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([16, 19, 59,  ..., 45, 61, 51])\n",
      "tensor([[0.0138, 0.0163, 0.0084,  ..., 0.0076, 0.0150, 0.0037],\n",
      "        [0.0172, 0.0182, 0.0087,  ..., 0.0103, 0.0165, 0.0031],\n",
      "        [0.0195, 0.0162, 0.0105,  ..., 0.0073, 0.0114, 0.0055],\n",
      "        ...,\n",
      "        [0.0230, 0.0153, 0.0090,  ..., 0.0073, 0.0101, 0.0047],\n",
      "        [0.0195, 0.0223, 0.0124,  ..., 0.0091, 0.0102, 0.0054],\n",
      "        [0.0135, 0.0229, 0.0122,  ..., 0.0093, 0.0140, 0.0041]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "torch.Size([5984, 64])\n",
      "Epoch 1/1 loss: 0.0006950927928169781\n",
      "tensor([16, 19, 59,  ..., 45, 61, 51])\n",
      "tensor([[0.0196, 0.0189, 0.0124,  ..., 0.0083, 0.0147, 0.0036],\n",
      "        [0.0215, 0.0160, 0.0093,  ..., 0.0122, 0.0143, 0.0048],\n",
      "        [0.0189, 0.0304, 0.0109,  ..., 0.0082, 0.0121, 0.0051],\n",
      "        ...,\n",
      "        [0.0213, 0.0182, 0.0122,  ..., 0.0131, 0.0139, 0.0050],\n",
      "        [0.0190, 0.0246, 0.0159,  ..., 0.0091, 0.0123, 0.0038],\n",
      "        [0.0167, 0.0271, 0.0141,  ..., 0.0079, 0.0181, 0.0038]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "torch.Size([5984, 64])\n",
      "Epoch 1/1 loss: 0.0013900239837360893\n",
      "tensor([16, 19, 59,  ..., 45, 61, 51])\n",
      "tensor([[0.0148, 0.0279, 0.0172,  ..., 0.0098, 0.0198, 0.0070],\n",
      "        [0.0115, 0.0213, 0.0187,  ..., 0.0153, 0.0233, 0.0060],\n",
      "        [0.0182, 0.0308, 0.0222,  ..., 0.0147, 0.0185, 0.0038],\n",
      "        ...,\n",
      "        [0.0178, 0.0277, 0.0169,  ..., 0.0102, 0.0248, 0.0048],\n",
      "        [0.0248, 0.0239, 0.0156,  ..., 0.0139, 0.0185, 0.0047],\n",
      "        [0.0163, 0.0279, 0.0152,  ..., 0.0181, 0.0158, 0.0043]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "torch.Size([5984, 64])\n",
      "Epoch 1/1 loss: 0.0020848430573621535\n",
      "tensor([16, 19, 59,  ..., 45, 61, 51])\n",
      "tensor([[0.0355, 0.0257, 0.0104,  ..., 0.0190, 0.0145, 0.0073],\n",
      "        [0.0213, 0.0201, 0.0151,  ..., 0.0162, 0.0304, 0.0043],\n",
      "        [0.0208, 0.0223, 0.0170,  ..., 0.0132, 0.0199, 0.0055],\n",
      "        ...,\n",
      "        [0.0154, 0.0300, 0.0180,  ..., 0.0107, 0.0218, 0.0037],\n",
      "        [0.0147, 0.0178, 0.0139,  ..., 0.0130, 0.0270, 0.0031],\n",
      "        [0.0179, 0.0238, 0.0156,  ..., 0.0165, 0.0291, 0.0044]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "torch.Size([5984, 64])\n",
      "Epoch 1/1 loss: 0.0027795329610294196\n",
      "tensor([16, 19, 59,  ..., 45, 61, 51])\n",
      "tensor([[0.0162, 0.0311, 0.0142,  ..., 0.0219, 0.0345, 0.0029],\n",
      "        [0.0151, 0.0237, 0.0209,  ..., 0.0139, 0.0313, 0.0032],\n",
      "        [0.0234, 0.0220, 0.0198,  ..., 0.0182, 0.0306, 0.0037],\n",
      "        ...,\n",
      "        [0.0265, 0.0162, 0.0145,  ..., 0.0109, 0.0205, 0.0078],\n",
      "        [0.0161, 0.0244, 0.0178,  ..., 0.0184, 0.0319, 0.0056],\n",
      "        [0.0217, 0.0151, 0.0118,  ..., 0.0142, 0.0262, 0.0038]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "torch.Size([5984, 64])\n",
      "Epoch 1/1 loss: 0.003474100786734392\n",
      "tensor([16, 19, 59,  ..., 45, 61, 51])\n",
      "tensor([[0.0193, 0.0287, 0.0216,  ..., 0.0179, 0.0190, 0.0037],\n",
      "        [0.0275, 0.0246, 0.0175,  ..., 0.0180, 0.0333, 0.0057],\n",
      "        [0.0228, 0.0283, 0.0204,  ..., 0.0144, 0.0184, 0.0046],\n",
      "        ...,\n",
      "        [0.0251, 0.0137, 0.0144,  ..., 0.0169, 0.0207, 0.0051],\n",
      "        [0.0162, 0.0232, 0.0175,  ..., 0.0167, 0.0345, 0.0022],\n",
      "        [0.0200, 0.0176, 0.0145,  ..., 0.0188, 0.0242, 0.0031]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "torch.Size([5984, 64])\n",
      "Epoch 1/1 loss: 0.004168571794734281\n",
      "tensor([16, 19, 59,  ..., 45, 61, 51])\n",
      "tensor([[0.0234, 0.0134, 0.0157,  ..., 0.0154, 0.0242, 0.0041],\n",
      "        [0.0153, 0.0199, 0.0169,  ..., 0.0176, 0.0246, 0.0023],\n",
      "        [0.0164, 0.0233, 0.0239,  ..., 0.0166, 0.0325, 0.0033],\n",
      "        ...,\n",
      "        [0.0203, 0.0166, 0.0162,  ..., 0.0138, 0.0309, 0.0044],\n",
      "        [0.0159, 0.0211, 0.0186,  ..., 0.0135, 0.0339, 0.0033],\n",
      "        [0.0127, 0.0166, 0.0174,  ..., 0.0286, 0.0371, 0.0036]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "torch.Size([5984, 64])\n",
      "Epoch 1/1 loss: 0.004862922477849664\n",
      "tensor([16, 19, 59,  ..., 45, 61, 51])\n",
      "tensor([[0.0227, 0.0112, 0.0164,  ..., 0.0150, 0.0424, 0.0034],\n",
      "        [0.0190, 0.0177, 0.0224,  ..., 0.0171, 0.0267, 0.0029],\n",
      "        [0.0174, 0.0201, 0.0205,  ..., 0.0122, 0.0279, 0.0027],\n",
      "        ...,\n",
      "        [0.0281, 0.0133, 0.0166,  ..., 0.0179, 0.0286, 0.0041],\n",
      "        [0.0178, 0.0230, 0.0225,  ..., 0.0212, 0.0200, 0.0028],\n",
      "        [0.0174, 0.0131, 0.0173,  ..., 0.0275, 0.0522, 0.0024]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "torch.Size([5984, 64])\n",
      "Epoch 1/1 loss: 0.0055571494096103196\n",
      "tensor([16, 19, 59,  ..., 45, 61, 51])\n",
      "tensor([[0.0227, 0.0115, 0.0181,  ..., 0.0170, 0.0256, 0.0026],\n",
      "        [0.0186, 0.0147, 0.0166,  ..., 0.0198, 0.0364, 0.0016],\n",
      "        [0.0143, 0.0226, 0.0282,  ..., 0.0267, 0.0367, 0.0030],\n",
      "        ...,\n",
      "        [0.0142, 0.0193, 0.0214,  ..., 0.0251, 0.0386, 0.0017],\n",
      "        [0.0169, 0.0111, 0.0130,  ..., 0.0178, 0.0280, 0.0020],\n",
      "        [0.0127, 0.0188, 0.0199,  ..., 0.0259, 0.0575, 0.0018]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "torch.Size([5984, 64])\n",
      "Epoch 1/1 loss: 0.006251243904312664\n",
      "tensor([16, 19, 59,  ..., 45, 61, 51])\n",
      "tensor([[0.0100, 0.0117, 0.0236,  ..., 0.0204, 0.0425, 0.0012],\n",
      "        [0.0077, 0.0083, 0.0188,  ..., 0.0221, 0.0412, 0.0007],\n",
      "        [0.0159, 0.0135, 0.0157,  ..., 0.0191, 0.0524, 0.0014],\n",
      "        ...,\n",
      "        [0.0180, 0.0152, 0.0247,  ..., 0.0218, 0.0408, 0.0019],\n",
      "        [0.0127, 0.0109, 0.0204,  ..., 0.0220, 0.0485, 0.0010],\n",
      "        [0.0174, 0.0112, 0.0188,  ..., 0.0164, 0.0479, 0.0017]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "torch.Size([5984, 64])\n",
      "Epoch 1/1 loss: 0.006945294651755674\n",
      "tensor([16, 19, 59,  ..., 45, 61, 51])\n",
      "tensor([[0.0095, 0.0108, 0.0293,  ..., 0.0184, 0.0630, 0.0011],\n",
      "        [0.0114, 0.0057, 0.0100,  ..., 0.0289, 0.0246, 0.0008],\n",
      "        [0.0121, 0.0110, 0.0195,  ..., 0.0171, 0.0452, 0.0014],\n",
      "        ...,\n",
      "        [0.0103, 0.0093, 0.0190,  ..., 0.0165, 0.0475, 0.0011],\n",
      "        [0.0089, 0.0070, 0.0190,  ..., 0.0197, 0.0248, 0.0013],\n",
      "        [0.0110, 0.0101, 0.0200,  ..., 0.0298, 0.0404, 0.0011]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "torch.Size([5984, 64])\n",
      "Epoch 1/1 loss: 0.007639317907751562\n",
      "tensor([16, 19, 59,  ..., 45, 61, 51])\n",
      "tensor([[0.0053, 0.0076, 0.0169,  ..., 0.0258, 0.0453, 0.0007],\n",
      "        [0.0119, 0.0087, 0.0257,  ..., 0.0175, 0.0439, 0.0022],\n",
      "        [0.0138, 0.0089, 0.0205,  ..., 0.0219, 0.0675, 0.0015],\n",
      "        ...,\n",
      "        [0.0100, 0.0073, 0.0144,  ..., 0.0324, 0.0460, 0.0008],\n",
      "        [0.0063, 0.0056, 0.0173,  ..., 0.0240, 0.0456, 0.0005],\n",
      "        [0.0064, 0.0112, 0.0191,  ..., 0.0185, 0.0592, 0.0008]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "torch.Size([5984, 64])\n",
      "Epoch 1/1 loss: 0.008333241636740332\n",
      "Validation Loss for model (0, 0): 0.002773840744334535\n",
      "Best model is from ensemble configuration with validation loss: 0.002773840744334535\n",
      "Best Model Validation Loss: 0.002773840744334535\n",
      "Best Model Validation Accuracy: 2.338009352037408%\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Loading data\n",
    "config = {\n",
    "    \"seed\": 0, \n",
    "    \"device\": \"cpu\", \n",
    "    \"features_dtype\": torch.float32,\n",
    "    \"ensemble_shape\": (1,1), \n",
    "    \"n_patches\": [8],  # 3 values for n_patches\n",
    "    \"hidden_layer_dim\": [36], # 5 values for hidden layer dimensions ( they are all divisible by 3 = n_head\n",
    "    \"n_heads\": 3}\n",
    "\n",
    "chess_features, chess_labels = torch.load('data/sample_dataset.pt')\n",
    "\n",
    "features_train, features_valid, labels_train, labels_valid = train_test_split(\n",
    "chess_features, chess_labels[:,0], test_size=0.2, random_state=42)\n",
    "\n",
    "\n",
    "\n",
    "train_dataset = TensorDataset(features_train, labels_train)\n",
    "train_loader = DataLoader(train_dataset, batch_size=512, shuffle=True)\n",
    "\n",
    "valid_dataset = TensorDataset(features_valid, labels_valid)\n",
    "valid_loader = DataLoader(valid_dataset, batch_size=128, shuffle=False)\n",
    "# Defining model and training options\n",
    "        \n",
    "device = config[\"device\"]\n",
    "N_EPOCHS = 1\n",
    "LR = 0.001\n",
    "# Here I want to find the best model out of the 15 models \n",
    "best_model = None\n",
    "best_loss = float('inf')\n",
    "\n",
    "# Training loop\n",
    "for n_patches_idx in range(config[\"ensemble_shape\"][0]):  \n",
    "    for hidden_layer_dim_idx in range(config[\"ensemble_shape\"][1]):  \n",
    "        ensemble_idx = (n_patches_idx, hidden_layer_dim_idx)\n",
    "        chw = (9, 8, 8)\n",
    "\n",
    "        model = MyViT(\n",
    "            chw=chw,\n",
    "            config=config,\n",
    "            ensemble_idx=ensemble_idx,\n",
    "            labels=labels_train,\n",
    "            n_blocks=3,\n",
    "            n_heads=3).to(device)\n",
    "        \n",
    "\n",
    "        optimizer = Adam(model.parameters(), lr=LR)\n",
    "        criterion = CrossEntropyLoss()\n",
    "\n",
    "        \n",
    "        for epoch in trange(N_EPOCHS, desc=\"Training\"):\n",
    "            train_loss = 0.0\n",
    "            model.train()\n",
    "            for batch in tqdm(train_loader, desc=f\"Epoch {epoch + 1} in training\", leave=False):\n",
    "                x, y = batch\n",
    "                x = features_train.to(device)\n",
    "                y = labels_train.to(device).to(torch.long)\n",
    "\n",
    "                y_hat = model(x)\n",
    "                # base = 100\n",
    "                # y_1dim = y[:, 0] * base + y[:, 1]\n",
    "                # y_1dim = y_1dim.to(torch.long)\n",
    "\n",
    "                print(y)\n",
    "\n",
    "                print(y_hat)\n",
    "\n",
    "                print(y_hat.shape)  # [batch_size, num_classes]\n",
    "                        \n",
    "                loss = criterion(y_hat, y)\n",
    "\n",
    "                train_loss += loss.detach().cpu().item() / len(features_train)\n",
    "\n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "                print(f\"Epoch {epoch + 1}/{N_EPOCHS} loss: {train_loss:}\")\n",
    "\n",
    "\n",
    "\n",
    "            # Validation phase\n",
    "            val_loss = 0.0\n",
    "            model.eval()\n",
    "            with torch.no_grad():\n",
    "                # for batch in valid_loader:\n",
    "                x = features_valid.to(device)\n",
    "                y = labels_valid.to(device).to(torch.long)\n",
    "\n",
    "                y_hat = model(x)\n",
    "                # base = 100\n",
    "                # y_1dim = y[:, 0] * base + y[:, 1]  \n",
    "                # loss = criterion(y_hat, y_1dim)\n",
    "\n",
    "\n",
    "                val_loss += loss.item() / len(features_valid)\n",
    "\n",
    "                print(f\"Validation Loss for model {ensemble_idx}: {val_loss}\")\n",
    "\n",
    "            # Check if this model is the best\n",
    "            if val_loss < best_loss:\n",
    "                best_loss = val_loss\n",
    "                best_model_state = model.state_dict()  # Save model state\n",
    "                best_ensemble_idx = ensemble_idx  # Optionally track the configuration\n",
    "\n",
    "print(f\"Best model is from ensemble configuration with validation loss: {best_loss}\")\n",
    "best_model = MyViT(\n",
    "    chw=(9,8,8),\n",
    "    config=config,\n",
    "    ensemble_idx=best_ensemble_idx,\n",
    "    labels=labels_train,\n",
    "    n_blocks=3,\n",
    "    n_heads=3).to(device)\n",
    "best_model.load_state_dict(best_model_state)\n",
    "# Test loop (Run the best model on the validation dataset)\n",
    "best_model.eval()\n",
    "val_loss = 0.0\n",
    "correct = 0\n",
    "total = 0\n",
    "\n",
    "with torch.no_grad():\n",
    "    # for batch in valid_loader:\n",
    "    x = features_valid.to(device)\n",
    "    y = labels_valid.to(device).to(torch.long)\n",
    "    y_hat = best_model(x)\n",
    "    # base = 100\n",
    "    # y_1dim = y[:, 0] * base + y[:, 1]       \n",
    "    # loss = criterion(y_hat, y_1dim)\n",
    "    val_loss += loss.item() / len(features_valid)\n",
    "\n",
    "        # Accuracy\n",
    "    _, predicted = torch.max(y_hat, 1) #finding the best class\n",
    "    correct += (predicted == y).sum().item()\n",
    "    total += y.size(0)\n",
    "print(f\"Best Model Validation Loss: {val_loss:}\")\n",
    "print(f\"Best Model Validation Accuracy: {100 * correct / total}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    # for batch in valid_loader:\n",
    "    x = features_train\n",
    "    y = labels_train.to(torch.long)\n",
    "    y_hat = best_model(x)\n",
    "    # Accuracy\n",
    "    _, predicted = torch.max(y_hat, 1) #finding the best class\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([11, 11, 11,  ..., 11, 11, 11])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_hat.argmax(dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.0267)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.mean((y.to(device) == y_hat.argmax(dim=-1).to(device)).type(torch.float32))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deep_learning",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  },
  "vscode": {
   "interpreter": {
    "hash": "c9c342e86147d9c01e8ada3827b0e2edcbb5d29938b6fc5705803e2841fa967f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
