{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datasets\n",
    "import transformers\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from datasets import load_dataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch.nn as nn\n",
    "\n",
    "from collections import defaultdict\n",
    "from collections.abc import (\n",
    "    Callable,\n",
    "    Iterable\n",
    ")\n",
    "import numpy as np\n",
    "\n",
    "from util_110724 import (\n",
    "    get_dataloader_random_reshuffle,\n",
    "    to_ensembled\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\n",
    "    \"seed\": 0, \n",
    "    \"device\": \"cuda\", \n",
    "    \"features_dtype\": torch.float32,\n",
    "    \"ensemble_shape\": (16,)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LayerNorm(torch.nn.Module):\n",
    "    \"\"\"\n",
    "    Ensemble-ready layer normalization layer\n",
    "\n",
    "    Arguments\n",
    "    ---------\n",
    "    config : `dict`\n",
    "        Configuration dictionary. Required key-value pairs:\n",
    "        `\"device\"` : `str`\n",
    "            The device to store parameters on.\n",
    "        `\"ensemble_shape\"` : `tuple[int]`\n",
    "            The shape of the ensemble of affine transformations\n",
    "            the model represents.\n",
    "        `\"float_dtype\"` : `torch.dtype`\n",
    "            The floating point datatype to use for the parameters.\n",
    "    normalized_shape : `int | tuple[int]`\n",
    "        The part of the shape of the incoming tensors\n",
    "        that are to be normalized together with batch dimensions.\n",
    "        We view the following as batch dimensions:\n",
    "        ```\n",
    "        range(\n",
    "            len(ensemble_shape),\n",
    "            -len(normalized_shape) - normalized_offset\n",
    "        )\n",
    "        ```\n",
    "        If an integer, we view it as a single-element tuple.\n",
    "    bias : `bool`, optional\n",
    "        If `elementwise_affine`, whether to include offset\n",
    "        in the learned transformation. Default: `True`.\n",
    "    elementwise_affine : `bool`, optional\n",
    "        Whether to include learnable scale. If this and `bias`,\n",
    "        then we also include learnable offset. These will be tensors\n",
    "        of shape `ensemble_shape + normalized_shape` that are\n",
    "        broadcast to the incoming feature tensors appropriately.\n",
    "        Default: `True`.\n",
    "    epsilon : `float`, optional\n",
    "        Small positive value, to be included in the divisor when we\n",
    "        divide by the variance, for numerical stability. Default: `1e-5`.\n",
    "    normalized_offset : `int`, optional\n",
    "        We get `normalized_shape` out of an incoming feature tensor\n",
    "        at dimensions\n",
    "        ```\n",
    "        range(\n",
    "            -len(normalized_shape) - normalized_offset,\n",
    "            -normalized_offset\n",
    "        )\n",
    "        ```\n",
    "        Default: `0`.\n",
    "\n",
    "    Calling\n",
    "    -------\n",
    "    Instance calls require one positional argument:\n",
    "    batch : `dict`\n",
    "        The input data dictionary. Required key:\n",
    "        `\"features\"` : `torch.Tensor`\n",
    "            Tensor of features.\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        config: dict,\n",
    "        normalized_shape: int | tuple[int],\n",
    "        bias=True,\n",
    "        elementwise_affine=True,\n",
    "        epsilon=1e-5,\n",
    "        normalized_offset=0\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        if hasattr(normalized_shape, \"__int__\"):\n",
    "            self.normalized_shape = (normalized_shape,)\n",
    "        else:\n",
    "            self.normalized_shape = normalized_shape\n",
    "\n",
    "        self.ensemble_shape = config[\"ensemble_shape\"]\n",
    "        self.epsilon = epsilon\n",
    "        self.normalized_offset = normalized_offset\n",
    "\n",
    "        if elementwise_affine:\n",
    "            self.scale = torch.nn.Parameter(torch.ones(\n",
    "                self.ensemble_shape + self.normalized_shape + (1,) * normalized_offset,\n",
    "                device=config[\"device\"],\n",
    "                dtype=config[\"float_dtype\"]\n",
    "            ))\n",
    "            if bias:\n",
    "                self.bias = torch.nn.Parameter(torch.zeros_like(self.scale))\n",
    "            else:\n",
    "                self.bias = None\n",
    "\n",
    "        else:\n",
    "            self.bias, self.scale = None, None\n",
    "\n",
    "\n",
    "    def forward(self, batch: dict) -> dict:\n",
    "        features: torch.Tensor = batch[\"features\"]\n",
    "\n",
    "        ensemble_dim = len(self.ensemble_shape)\n",
    "        features = to_ensembled(self.ensemble_shape, features)\n",
    "\n",
    "        normalized_dim = len(self.normalized_shape)\n",
    "\n",
    "        batch_dim = len(features.shape) - ensemble_dim - normalized_dim - self.normalized_offset\n",
    "        normalized_range = tuple(range(\n",
    "            ensemble_dim,\n",
    "            ensemble_dim + batch_dim\n",
    "        )) + tuple(range(\n",
    "            -normalized_dim - self.normalized_offset,\n",
    "            -self.normalized_offset\n",
    "        ))\n",
    "\n",
    "        features = features - features.mean(dim=normalized_range, keepdim=True)\n",
    "        features = features / features.std(dim=normalized_range, keepdim=True)\n",
    "\n",
    "        if self.scale is not None:\n",
    "            scale = self.scale.unflatten(\n",
    "                ensemble_dim,\n",
    "                (1,) * batch_dim + self.normalized_shape[:1]\n",
    "            )\n",
    "\n",
    "            features = features * scale\n",
    "\n",
    "            if self.bias is not None:\n",
    "                bias = self.bias.unflatten(\n",
    "                    ensemble_dim,\n",
    "                    (1,) * batch_dim + self.normalized_shape[:1]\n",
    "                )\n",
    "                features = features + bias\n",
    "\n",
    "        return batch | {\"features\": features}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DML",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
